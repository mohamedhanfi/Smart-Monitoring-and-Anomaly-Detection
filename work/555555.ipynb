{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cee84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox, simpledialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tkinter import PhotoImage\n",
    "import numpy as np\n",
    "\n",
    "# Import the model loader from TensorFlow/Keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Constants for prediction ===\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64   # Preprocessing dimensions for model input\n",
    "SEQUENCE_LENGTH = 16                 # Number of frames per prediction sequence\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]\n",
    "\n",
    "# Define the target camera index which runs the model inference\n",
    "TARGET_CAMERA = 0\n",
    "\n",
    "# --- Helper Function ---\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resize and normalize a single frame for model input.\"\"\"\n",
    "    frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "class SmartMonitoringApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Smart Monitoring & Anomaly Detection\")\n",
    "        self.root.state('zoomed')  # Start in full-screen mode\n",
    "\n",
    "        # Load background image (update path as necessary)\n",
    "        self.bg_image_path = \"assests/111.jpg\"  \n",
    "        self.bg_image = Image.open(self.bg_image_path)\n",
    "        self.bg_image = self.bg_image.resize((self.root.winfo_screenwidth(), self.root.winfo_screenheight()))\n",
    "        self.bg_photo = ImageTk.PhotoImage(self.bg_image)\n",
    "\n",
    "        # Set the background image\n",
    "        self.bg_label = tk.Label(self.root, image=self.bg_photo)\n",
    "        self.bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "        # Load users from a file (for login)\n",
    "        self.users_file = \"users.txt\"\n",
    "        self.users = self.load_users()\n",
    "\n",
    "        # Setup styling for the application\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        style.configure(\"TFrame\", background=\"black\")\n",
    "        style.configure(\"TLabel\", background=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TEntry\", fieldbackground=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TButton\", background=\"black\", foreground=\"white\")\n",
    "        self.root.configure(bg=\"black\")\n",
    "\n",
    "        # Create login frame\n",
    "        self.login_frame = ttk.Frame(self.root, padding=20, style=\"TFrame\")\n",
    "        self.login_frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "\n",
    "        # Load icons for username and password (update paths as needed)\n",
    "        self.user_icon = PhotoImage(file=\"assests/icons8-male-user-50.png\")\n",
    "        self.password_icon = PhotoImage(file=\"assests/icons8-password-48.png\")\n",
    "\n",
    "        # Username and password fields\n",
    "        self.username_label = ttk.Label(self.login_frame, text=\"Username:\", font=(\"Arial\", 14))\n",
    "        self.username_label.grid(row=0, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.user_icon_label = ttk.Label(self.login_frame, image=self.user_icon, background=\"black\")\n",
    "        self.user_icon_label.grid(row=0, column=1, padx=(0, 10))\n",
    "        self.username_entry = ttk.Entry(self.login_frame, font=(\"Arial\", 14))\n",
    "        self.username_entry.grid(row=0, column=2, padx=(0, 10), pady=15)\n",
    "        self.password_label = ttk.Label(self.login_frame, text=\"Password:\", font=(\"Arial\", 14))\n",
    "        self.password_label.grid(row=1, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.password_icon_label = ttk.Label(self.login_frame, image=self.password_icon, background=\"black\")\n",
    "        self.password_icon_label.grid(row=1, column=1, padx=(0, 10))\n",
    "        self.password_entry = ttk.Entry(self.login_frame, show=\"*\", font=(\"Arial\", 14))\n",
    "        self.password_entry.grid(row=1, column=2, padx=(0, 10), pady=15)\n",
    "        self.login_button = ttk.Button(self.login_frame, text=\"Login\", command=self.login)\n",
    "        self.login_button.grid(row=2, column=1, columnspan=2, padx=20, pady=15)\n",
    "\n",
    "        # Main application frames\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.report_frame = ttk.Frame(root)\n",
    "        self.admin_frame = ttk.Frame(root)\n",
    "\n",
    "        # Initialize camera captures for 8 cameras.\n",
    "        # For camera 0 (TARGET_CAMERA): load the specific video for model inference.\n",
    "        # For all other cameras, set to None (disabled).\n",
    "        self.num_cameras = 8\n",
    "        self.captures = []\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA:\n",
    "                # Load the specific video for camera 0 (update the file path accordingly)\n",
    "                cap = cv2.VideoCapture(\"videos/BigFight.mp4\")\n",
    "            else:\n",
    "                cap = None\n",
    "            self.captures.append(cap)\n",
    "\n",
    "        # Create labels to display camera feeds.\n",
    "        self.camera_labels = []\n",
    "\n",
    "        # Report Panel (for prediction logs)\n",
    "        self.report_listbox = tk.Listbox(self.report_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                         background=\"black\", fg=\"white\")\n",
    "        self.report_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Admin/Operator Panel components (if needed)\n",
    "        self.operator_listbox = tk.Listbox(self.admin_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                           background=\"black\", fg=\"white\")\n",
    "        self.operator_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "        self.add_operator_button = ttk.Button(self.admin_frame, text=\"Add Operator\", command=self.add_operator)\n",
    "        self.add_operator_button.pack(padx=10, pady=10)\n",
    "        self.delete_operator_button = ttk.Button(self.admin_frame, text=\"Delete Operator\", command=self.delete_operator)\n",
    "        self.delete_operator_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # \"Go Back\" button to return to the login interface.\n",
    "        self.go_back_button = ttk.Button(self.root, text=\"Go Back\", command=self.go_back)\n",
    "        self.go_back_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Load the model (MobileNetV2+biLSTM) from file.\n",
    "        try:\n",
    "            self.model = load_model(\"models/mobileNetv2_biLSTM.h5\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Model Load Error\", f\"Failed to load model: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "        # Variables for prediction logging and frame buffering.\n",
    "        self.last_prediction = None\n",
    "        self.last_report_time = 0\n",
    "        self.frames_buffer = []  # Buffer for the last SEQUENCE_LENGTH preprocessed frames\n",
    "\n",
    "    def load_users(self):\n",
    "        \"\"\"Loads users from file into a dictionary.\"\"\"\n",
    "        users = {}\n",
    "        try:\n",
    "            with open(self.users_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    username, password, role = line.strip().split(\",\")\n",
    "                    users[username] = {\"password\": password, \"role\": role}\n",
    "        except FileNotFoundError:\n",
    "            with open(self.users_file, \"w\") as file:\n",
    "                file.write(\"admin,admin123,admin\\n\")\n",
    "            users = {\"admin\": {\"password\": \"admin123\", \"role\": \"admin\"}}\n",
    "        return users\n",
    "\n",
    "    def save_user(self, username, password, role):\n",
    "        \"\"\"Saves a new user to file.\"\"\"\n",
    "        with open(self.users_file, \"a\") as file:\n",
    "            file.write(f\"{username},{password},{role}\\n\")\n",
    "\n",
    "    def delete_user(self, username):\n",
    "        \"\"\"Deletes a user from file.\"\"\"\n",
    "        with open(self.users_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        with open(self.users_file, \"w\") as file:\n",
    "            for line in lines:\n",
    "                if not line.startswith(username + \",\"):\n",
    "                    file.write(line)\n",
    "\n",
    "    def login(self):\n",
    "        username = self.username_entry.get()\n",
    "        password = self.password_entry.get()\n",
    "        if not re.match(\"^[A-Za-z]+$\", username):\n",
    "            messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "            return\n",
    "        if len(password) < 8:\n",
    "            messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        if username in self.users and self.users[username][\"password\"] == password:\n",
    "            self.login_frame.destroy()\n",
    "            if self.users[username][\"role\"] == \"admin\":\n",
    "                self.show_admin_interface()\n",
    "            else:\n",
    "                self.show_operator_interface()\n",
    "        else:\n",
    "            messagebox.showerror(\"Login Failed\", \"Invalid username or password\")\n",
    "\n",
    "    def add_operator(self):\n",
    "        username = simpledialog.askstring(\"Add Operator\", \"Enter username:\")\n",
    "        if username:\n",
    "            if not re.match(\"^[A-Za-z]+$\", username):\n",
    "                messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "                return\n",
    "            if username in self.users:\n",
    "                messagebox.showerror(\"Error\", \"Username already exists!\")\n",
    "                return\n",
    "            password = simpledialog.askstring(\"Add Operator\", \"Enter password:\", show=\"*\")\n",
    "            if password:\n",
    "                if len(password) < 8:\n",
    "                    messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "                    return\n",
    "                self.save_user(username, password, \"operator\")\n",
    "                self.users[username] = {\"password\": password, \"role\": \"operator\"}\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' added successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Password cannot be empty!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Username cannot be empty!\")\n",
    "\n",
    "    def update_operator_listbox(self):\n",
    "        self.operator_listbox.delete(0, tk.END)\n",
    "        for username, info in self.users.items():\n",
    "            if info[\"role\"] == \"operator\":\n",
    "                self.operator_listbox.insert(tk.END, username)\n",
    "\n",
    "    def delete_operator(self):\n",
    "        selected = self.operator_listbox.curselection()\n",
    "        if selected:\n",
    "            username = self.operator_listbox.get(selected)\n",
    "            if username in self.users:\n",
    "                self.delete_user(username)\n",
    "                del self.users[username]\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' deleted successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Operator not found!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"No operator selected!\")\n",
    "\n",
    "    def show_admin_interface(self):\n",
    "        self.admin_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.update_operator_listbox()\n",
    "\n",
    "    def show_operator_interface(self):\n",
    "        self.main_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.create_camera_grid()\n",
    "        self.start_video_threads()\n",
    "\n",
    "    def create_camera_grid(self):\n",
    "        \"\"\"Creates a grid layout for displaying camera feeds.\"\"\"\n",
    "        rows, cols = 4, 2\n",
    "        for i in range(self.num_cameras):\n",
    "            frame = ttk.LabelFrame(self.main_frame, text=f\"Camera {i+1}\")\n",
    "            frame.grid(row=i // cols, column=i % cols, padx=10, pady=10, sticky=\"nsew\")\n",
    "            label = tk.Label(frame, text=\"Initializing...\", font=(\"Arial\", 14), fg=\"red\")\n",
    "            label.pack(fill=tk.BOTH, expand=True)\n",
    "            self.camera_labels.append(label)\n",
    "        for i in range(rows):\n",
    "            self.main_frame.grid_rowconfigure(i, weight=1)\n",
    "        for j in range(cols):\n",
    "            self.main_frame.grid_columnconfigure(j, weight=1)\n",
    "\n",
    "    def start_video_threads(self):\n",
    "        \"\"\"\n",
    "        Starts the appropriate thread for each camera:\n",
    "        - Camera 0 (TARGET_CAMERA) runs update_camera (with model inference using the specific video).\n",
    "        - All other cameras are disabled.\n",
    "        \"\"\"\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA:\n",
    "                threading.Thread(target=self.update_camera, args=(i,), daemon=True).start()\n",
    "            else:\n",
    "                self.camera_labels[i].config(text=\"Camera Disabled\", font=(\"Arial\", 16), fg=\"yellow\")\n",
    "\n",
    "    def predict_violence(self, frames_list):\n",
    "        \"\"\"\n",
    "        Runs model prediction over the last SEQUENCE_LENGTH frames.\n",
    "        Returns the predicted class label.\n",
    "        \"\"\"\n",
    "        input_frames = np.array([frames_list[-SEQUENCE_LENGTH:]])\n",
    "        prediction = self.model.predict(input_frames)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        return CLASSES_LIST[predicted_class]\n",
    "\n",
    "    def update_camera(self, index):\n",
    "        \"\"\"\n",
    "        Reads frames from camera 0 (the specific video),\n",
    "        buffers them for sequence prediction,\n",
    "        overlays prediction text on the display frame,\n",
    "        and logs the result to the report panel.\n",
    "        \n",
    "        *Note:* Predictions with label \"NonViolence\" are not logged.\n",
    "        \"\"\"\n",
    "        if self.captures[index] is None:\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                # Restart video if we reach the end.\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "\n",
    "            # Convert BGR to RGB and resize for display.\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            display_frame = cv2.resize(rgb_frame, (400, 300))\n",
    "            \n",
    "            # Preprocess the frame for prediction.\n",
    "            proc_frame = preprocess_frame(rgb_frame)\n",
    "            self.frames_buffer.append(proc_frame)\n",
    "            if len(self.frames_buffer) > SEQUENCE_LENGTH:\n",
    "                self.frames_buffer = self.frames_buffer[-SEQUENCE_LENGTH:]\n",
    "\n",
    "            # Run prediction if enough frames exist.\n",
    "            if len(self.frames_buffer) >= SEQUENCE_LENGTH and self.model is not None:\n",
    "                pred_label = self.predict_violence(self.frames_buffer)\n",
    "                # cv2.putText(display_frame, f\"Prediction: {pred_label}\", (10, 25),\n",
    "                #             cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "                current_time = time.time()\n",
    "                # Only log predictions if they are not \"NonViolence\"\n",
    "                # and if the prediction changed or 5 seconds have elapsed.\n",
    "                if pred_label != \"NonViolence\" and (self.last_prediction != pred_label or (current_time - self.last_report_time > 5)):\n",
    "                    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    report = f\"[{timestamp}] Prediction: {pred_label}\"\n",
    "                    self.report_listbox.insert(tk.END, report)\n",
    "                    self.report_listbox.insert(tk.END, \"-\" * 50)\n",
    "                    self.last_prediction = pred_label\n",
    "                    self.last_report_time = current_time\n",
    "            else:\n",
    "                cv2.putText(display_frame, \"Loading...\", (10, 25),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(display_frame))\n",
    "            self.camera_labels[index].imgtk = img\n",
    "            self.camera_labels[index].config(image=img)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def go_back(self):\n",
    "        \"\"\"Resets the UI and returns to the login page.\"\"\"\n",
    "        for widget in self.root.winfo_children():\n",
    "            widget.destroy()\n",
    "        self.__init__(self.root)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'captures'):\n",
    "            for cap in self.captures:\n",
    "                if cap and cap.isOpened():\n",
    "                    cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = SmartMonitoringApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcf7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Accident, 198.9ms\n",
      "Speed: 2.0ms preprocess, 198.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 198.6ms\n",
      "Speed: 1.9ms preprocess, 198.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "0: 384x640 1 Accident, 286.7ms\n",
      "Speed: 1.8ms preprocess, 286.7ms inference, 12.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 402.2ms\n",
      "Speed: 2.8ms preprocess, 402.2ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 327.8ms\n",
      "Speed: 3.3ms preprocess, 327.8ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 342.3ms\n",
      "Speed: 5.9ms preprocess, 342.3ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 340.9ms\n",
      "Speed: 2.5ms preprocess, 340.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 194.7ms\n",
      "Speed: 1.7ms preprocess, 194.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 197.7ms\n",
      "Speed: 1.7ms preprocess, 197.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 193.4ms\n",
      "Speed: 1.7ms preprocess, 193.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 194.5ms\n",
      "Speed: 1.8ms preprocess, 194.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step0: 384x640 (no detections), 197.6ms\n",
      "Speed: 1.8ms preprocess, 197.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "0: 384x640 1 Accident, 218.9ms\n",
      "Speed: 1.6ms preprocess, 218.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "0: 384x640 1 Accident, 226.8ms\n",
      "Speed: 1.9ms preprocess, 226.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 (no detections), 235.7ms\n",
      "Speed: 1.7ms preprocess, 235.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "0: 384x640 1 Accident, 219.0ms\n",
      "Speed: 1.3ms preprocess, 219.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "0: 384x640 1 Accident, 218.7ms\n",
      "Speed: 1.9ms preprocess, 218.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "0: 384x640 1 Accident, 213.0ms\n",
      "Speed: 2.2ms preprocess, 213.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "0: 384x640 1 Accident, 222.8ms\n",
      "Speed: 2.1ms preprocess, 222.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "0: 384x640 1 Accident, 220.9ms\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/stepSpeed: 4.8ms preprocess, 220.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "0: 384x640 1 Accident, 230.5ms\n",
      "Speed: 1.5ms preprocess, 230.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "0: 384x640 2 Accidents, 226.5ms\n",
      "Speed: 1.5ms preprocess, 226.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "0: 384x640 2 Accidents, 225.6ms\n",
      "Speed: 1.5ms preprocess, 225.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "0: 384x640 2 Accidents, 226.6ms\n",
      "Speed: 1.6ms preprocess, 226.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "0: 384x640 2 Accidents, 227.5ms\n",
      "Speed: 1.7ms preprocess, 227.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "0: 384x640 2 Accidents, 217.3ms\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/stepSpeed: 1.8ms preprocess, 217.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 2 Accidents, 220.6ms\n",
      "Speed: 1.1ms preprocess, 220.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "0: 384x640 2 Accidents, 220.9ms\n",
      "Speed: 1.6ms preprocess, 220.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "0: 384x640 2 Accidents, 209.7ms\n",
      "Speed: 2.0ms preprocess, 209.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "0: 384x640 2 Accidents, 217.5ms\n",
      "Speed: 2.0ms preprocess, 217.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step0: 384x640 2 Accidents, 216.8ms\n",
      "Speed: 1.7ms preprocess, 216.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "0: 384x640 2 Accidents, 220.0ms\n",
      "Speed: 3.4ms preprocess, 220.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "0: 384x640 2 Accidents, 219.0ms\n",
      "Speed: 1.5ms preprocess, 219.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "0: 384x640 2 Accidents, 235.2ms\n",
      "Speed: 2.0ms preprocess, 235.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "0: 384x640 2 Accidents, 211.2ms\n",
      "Speed: 1.7ms preprocess, 211.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "0: 384x640 2 Accidents, 212.6ms\n",
      "Speed: 1.8ms preprocess, 212.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 228.1ms\n",
      "Speed: 1.8ms preprocess, 228.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 216.2ms\n",
      "Speed: 2.6ms preprocess, 216.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "0: 384x640 2 Accidents, 219.2ms\n",
      "Speed: 1.6ms preprocess, 219.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "0: 384x640 2 Accidents, 220.9ms\n",
      "Speed: 1.6ms preprocess, 220.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "0: 384x640 2 Accidents, 221.9ms\n",
      "Speed: 1.5ms preprocess, 221.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 216.9ms\n",
      "Speed: 1.7ms preprocess, 216.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "0: 384x640 2 Accidents, 228.3ms\n",
      "Speed: 1.8ms preprocess, 228.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 214.5ms\n",
      "Speed: 1.9ms preprocess, 214.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step0: 384x640 2 Accidents, 219.6ms\n",
      "Speed: 1.7ms preprocess, 219.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "0: 384x640 2 Accidents, 217.6ms\n",
      "Speed: 4.2ms preprocess, 217.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "0: 384x640 2 Accidents, 225.3ms\n",
      "Speed: 1.4ms preprocess, 225.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "0: 384x640 2 Accidents, 220.1ms\n",
      "Speed: 1.3ms preprocess, 220.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 2 Accidents, 219.6ms\n",
      "Speed: 1.6ms preprocess, 219.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 232.9ms\n",
      "Speed: 1.5ms preprocess, 232.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 206.8ms\n",
      "Speed: 1.8ms preprocess, 206.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 218.9ms\n",
      "Speed: 2.2ms preprocess, 218.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "0: 384x640 2 Accidents, 233.4ms\n",
      "Speed: 1.9ms preprocess, 233.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 230.4ms\n",
      "Speed: 1.5ms preprocess, 230.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "0: 384x640 2 Accidents, 221.2ms\n",
      "Speed: 1.5ms preprocess, 221.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "0: 384x640 2 Accidents, 219.8ms\n",
      "Speed: 1.4ms preprocess, 219.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "0: 384x640 2 Accidents, 209.1ms\n",
      "Speed: 1.8ms preprocess, 209.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 2 Accidents, 217.5ms\n",
      "Speed: 1.9ms preprocess, 217.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step0: 384x640 2 Accidents, 221.3ms\n",
      "Speed: 2.1ms preprocess, 221.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 222.8ms\n",
      "Speed: 1.5ms preprocess, 222.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 229.3ms\n",
      "Speed: 1.4ms preprocess, 229.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "0: 384x640 2 Accidents, 210.1ms\n",
      "Speed: 1.8ms preprocess, 210.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 221.2ms\n",
      "Speed: 1.9ms preprocess, 221.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step0: 384x640 2 Accidents, 226.3ms\n",
      "Speed: 1.6ms preprocess, 226.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 1 Accident, 224.9ms\n",
      "Speed: 1.9ms preprocess, 224.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 2 Accidents, 227.6ms\n",
      "Speed: 1.5ms preprocess, 227.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "0: 384x640 1 Accident, 234.7ms\n",
      "Speed: 1.6ms preprocess, 234.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "0: 384x640 1 Accident, 219.6ms\n",
      "Speed: 1.7ms preprocess, 219.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "0: 384x640 1 Accident, 197.6ms\n",
      "Speed: 1.6ms preprocess, 197.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\medoo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    self.run()\n",
      "  File \"C:\\Users\\medoo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 892, in run\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 892, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\medoo\\AppData\\Local\\Temp\\ipykernel_20420\\1936425889.py\", line 328, in update_camera\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\medoo\\AppData\\Local\\Temp\\ipykernel_20420\\1936425889.py\", line 393, in update_camera_yolo\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\ImageTk.py\", line 128, in __init__\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py\", line 3197, in insert\n",
      "    self.__photo = tkinter.PhotoImage(**kw)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py\", line 4064, in __init__\n",
      "    self.tk.call((self._w, 'insert', index) + elements)\n",
      "_tkinter.TclError: invalid command name \".!frame3.!listbox\"\n",
      "    Image.__init__(self, 'photo', name, cnf, master, **kw)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py\", line 4009, in __init__\n",
      "    self.tk.call(('image', 'create', imgtype, name,) + options)\n",
      "_tkinter.TclError: can't invoke \"image\" command: application has been destroyed\n",
      "Exception ignored in: <function Image.__del__ at 0x000001E990E69700>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py\", line 4017, in __del__\n",
      "    self.tk.call('image', 'delete', self.name)\n",
      "RuntimeError: main thread is not in main loop\n",
      "Exception ignored in: <function Image.__del__ at 0x000001E990E69700>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py\", line 4017, in __del__\n",
      "    self.tk.call('image', 'delete', self.name)\n",
      "RuntimeError: main thread is not in main loop\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox, simpledialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tkinter import PhotoImage\n",
    "import numpy as np\n",
    "\n",
    "# Import for model1 (MobileNetV2+biLSTM)\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Import YOLO model from Ultralytics for model2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# === Constants for model1 prediction (MobileNetV2+biLSTM) ===\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64   # Preprocessing dimensions for model1 input\n",
    "SEQUENCE_LENGTH = 16                 # Number of frames per prediction sequence\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]\n",
    "\n",
    "# Define target camera indices\n",
    "TARGET_CAMERA_MODEL1 = 0   # For model1 (MobileNetV2+biLSTM)\n",
    "TARGET_CAMERA_YOLO   = 1   # For model2 (YOLO)\n",
    "\n",
    "# --- Helper Function for model1 ---\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resize and normalize a single frame for model1 input.\"\"\"\n",
    "    frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "class SmartMonitoringApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Smart Monitoring & Anomaly Detection\")\n",
    "        self.root.state('zoomed')  # Start in full-screen mode\n",
    "\n",
    "        # Load background image (update path as necessary)\n",
    "        self.bg_image_path = \"assests/111.jpg\"  \n",
    "        self.bg_image = Image.open(self.bg_image_path)\n",
    "        self.bg_image = self.bg_image.resize((self.root.winfo_screenwidth(), self.root.winfo_screenheight()))\n",
    "        self.bg_photo = ImageTk.PhotoImage(self.bg_image)\n",
    "        self.bg_label = tk.Label(self.root, image=self.bg_photo)\n",
    "        self.bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "        # Load users from a file (for login)\n",
    "        self.users_file = \"users.txt\"\n",
    "        self.users = self.load_users()\n",
    "\n",
    "        # Setup styling for the application\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        style.configure(\"TFrame\", background=\"black\")\n",
    "        style.configure(\"TLabel\", background=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TEntry\", fieldbackground=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TButton\", background=\"black\", foreground=\"white\")\n",
    "        self.root.configure(bg=\"black\")\n",
    "\n",
    "        # Create login frame\n",
    "        self.login_frame = ttk.Frame(self.root, padding=20, style=\"TFrame\")\n",
    "        self.login_frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "\n",
    "        # Load icons for username and password (update paths as needed)\n",
    "        self.user_icon = PhotoImage(file=\"assests/icons8-male-user-50.png\")\n",
    "        self.password_icon = PhotoImage(file=\"assests/icons8-password-48.png\")\n",
    "\n",
    "        # Username and password fields\n",
    "        self.username_label = ttk.Label(self.login_frame, text=\"Username:\", font=(\"Arial\", 14))\n",
    "        self.username_label.grid(row=0, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.user_icon_label = ttk.Label(self.login_frame, image=self.user_icon, background=\"black\")\n",
    "        self.user_icon_label.grid(row=0, column=1, padx=(0, 10))\n",
    "        self.username_entry = ttk.Entry(self.login_frame, font=(\"Arial\", 14))\n",
    "        self.username_entry.grid(row=0, column=2, padx=(0, 10), pady=15)\n",
    "        self.password_label = ttk.Label(self.login_frame, text=\"Password:\", font=(\"Arial\", 14))\n",
    "        self.password_label.grid(row=1, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.password_icon_label = ttk.Label(self.login_frame, image=self.password_icon, background=\"black\")\n",
    "        self.password_icon_label.grid(row=1, column=1, padx=(0, 10))\n",
    "        self.password_entry = ttk.Entry(self.login_frame, show=\"*\", font=(\"Arial\", 14))\n",
    "        self.password_entry.grid(row=1, column=2, padx=(0, 10), pady=15)\n",
    "        self.login_button = ttk.Button(self.login_frame, text=\"Login\", command=self.login)\n",
    "        self.login_button.grid(row=2, column=1, columnspan=2, padx=20, pady=15)\n",
    "\n",
    "        # Main application frames\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.report_frame = ttk.Frame(root)\n",
    "        self.admin_frame = ttk.Frame(root)\n",
    "\n",
    "        # Initialize camera captures for 8 cameras.\n",
    "        # For camera 0: load video for model1; for camera 1: load video for model2.\n",
    "        self.num_cameras = 8\n",
    "        self.captures = []\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                # Video for model1 on camera 0 (update file path accordingly)\n",
    "                cap = cv2.VideoCapture(\"videos/BigFight.mp4\")\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                # Video for YOLO on camera 1 (update file path accordingly)\n",
    "                cap = cv2.VideoCapture(\"videos/cr.mp4\")\n",
    "            else:\n",
    "                cap = None\n",
    "            self.captures.append(cap)\n",
    "\n",
    "        # Create labels to display camera feeds.\n",
    "        self.camera_labels = []\n",
    "\n",
    "        # Report Panel (for logging predictions/detections)\n",
    "        self.report_listbox = tk.Listbox(self.report_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                         background=\"black\", fg=\"white\")\n",
    "        self.report_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Admin/Operator Panel components (if needed)\n",
    "        self.operator_listbox = tk.Listbox(self.admin_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                           background=\"black\", fg=\"white\")\n",
    "        self.operator_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "        self.add_operator_button = ttk.Button(self.admin_frame, text=\"Add Operator\", command=self.add_operator)\n",
    "        self.add_operator_button.pack(padx=10, pady=10)\n",
    "        self.delete_operator_button = ttk.Button(self.admin_frame, text=\"Delete Operator\", command=self.delete_operator)\n",
    "        self.delete_operator_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # \"Go Back\" button to return to the login interface.\n",
    "        self.go_back_button = ttk.Button(self.root, text=\"Go Back\", command=self.go_back)\n",
    "        self.go_back_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Load model1 (MobileNetV2+biLSTM) from file.\n",
    "        try:\n",
    "            self.model = load_model(\"models/mobileNetv2_biLSTM.h5\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Model1 Load Error\", f\"Failed to load model1: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "        # Load model2 (YOLO) using Ultralytics.\n",
    "        try:\n",
    "            self.yolo_model = YOLO('models/best.pt')\n",
    "            # YOLO model from Ultralytics typically provides model.names (a dict mapping class indices to names)\n",
    "            self.yolo_names = self.yolo_model.names  # This may be a dict: {0: 'person', 1: 'bicycle', ...}\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"YOLO Model Load Error\", f\"Failed to load YOLO model: {e}\")\n",
    "            self.yolo_model = None\n",
    "            self.yolo_names = {}\n",
    "\n",
    "        # Variables for model1 prediction logging and frame buffering.\n",
    "        self.last_prediction = None\n",
    "        self.last_report_time = 0\n",
    "        self.frames_buffer = []  # Buffer for the last SEQUENCE_LENGTH preprocessed frames\n",
    "        # Variables for YOLO model logging.\n",
    "        self.last_yolo_report_time = 0\n",
    "        self.last_yolo_detections = None\n",
    "\n",
    "    def load_users(self):\n",
    "        \"\"\"Loads users from file into a dictionary.\"\"\"\n",
    "        users = {}\n",
    "        try:\n",
    "            with open(self.users_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    username, password, role = line.strip().split(\",\")\n",
    "                    users[username] = {\"password\": password, \"role\": role}\n",
    "        except FileNotFoundError:\n",
    "            with open(self.users_file, \"w\") as file:\n",
    "                file.write(\"admin,admin123,admin\\n\")\n",
    "            users = {\"admin\": {\"password\": \"admin123\", \"role\": \"admin\"}}\n",
    "        return users\n",
    "\n",
    "    def save_user(self, username, password, role):\n",
    "        \"\"\"Saves a new user to file.\"\"\"\n",
    "        with open(self.users_file, \"a\") as file:\n",
    "            file.write(f\"{username},{password},{role}\\n\")\n",
    "\n",
    "    def delete_user(self, username):\n",
    "        \"\"\"Deletes a user from file.\"\"\"\n",
    "        with open(self.users_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        with open(self.users_file, \"w\") as file:\n",
    "            for line in lines:\n",
    "                if not line.startswith(username + \",\"):\n",
    "                    file.write(line)\n",
    "\n",
    "    def login(self):\n",
    "        username = self.username_entry.get()\n",
    "        password = self.password_entry.get()\n",
    "        if not re.match(\"^[A-Za-z]+$\", username):\n",
    "            messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "            return\n",
    "        if len(password) < 8:\n",
    "            messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        if username in self.users and self.users[username][\"password\"] == password:\n",
    "            self.login_frame.destroy()\n",
    "            if self.users[username][\"role\"] == \"admin\":\n",
    "                self.show_admin_interface()\n",
    "            else:\n",
    "                self.show_operator_interface()\n",
    "        else:\n",
    "            messagebox.showerror(\"Login Failed\", \"Invalid username or password\")\n",
    "\n",
    "    def add_operator(self):\n",
    "        username = simpledialog.askstring(\"Add Operator\", \"Enter username:\")\n",
    "        if username:\n",
    "            if not re.match(\"^[A-Za-z]+$\", username):\n",
    "                messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "                return\n",
    "            if username in self.users:\n",
    "                messagebox.showerror(\"Error\", \"Username already exists!\")\n",
    "                return\n",
    "            password = simpledialog.askstring(\"Add Operator\", \"Enter password:\", show=\"*\")\n",
    "            if password:\n",
    "                if len(password) < 8:\n",
    "                    messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "                    return\n",
    "                self.save_user(username, password, \"operator\")\n",
    "                self.users[username] = {\"password\": password, \"role\": \"operator\"}\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' added successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Password cannot be empty!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Username cannot be empty!\")\n",
    "\n",
    "    def update_operator_listbox(self):\n",
    "        self.operator_listbox.delete(0, tk.END)\n",
    "        for username, info in self.users.items():\n",
    "            if info[\"role\"] == \"operator\":\n",
    "                self.operator_listbox.insert(tk.END, username)\n",
    "\n",
    "    def delete_user_from_list(self):\n",
    "        selected = self.operator_listbox.curselection()\n",
    "        if selected:\n",
    "            username = self.operator_listbox.get(selected)\n",
    "            if username in self.users:\n",
    "                self.delete_user(username)\n",
    "                del self.users[username]\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' deleted successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Operator not found!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"No operator selected!\")\n",
    "\n",
    "    def delete_operator(self):\n",
    "        self.delete_user_from_list()\n",
    "\n",
    "    def show_admin_interface(self):\n",
    "        self.admin_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.update_operator_listbox()\n",
    "\n",
    "    def show_operator_interface(self):\n",
    "        self.main_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.create_camera_grid()\n",
    "        self.start_video_threads()\n",
    "\n",
    "    def create_camera_grid(self):\n",
    "        \"\"\"Creates a grid layout for displaying camera feeds.\"\"\"\n",
    "        rows, cols = 4, 2\n",
    "        for i in range(self.num_cameras):\n",
    "            frame = ttk.LabelFrame(self.main_frame, text=f\"Camera {i+1}\")\n",
    "            frame.grid(row=i // cols, column=i % cols, padx=10, pady=10, sticky=\"nsew\")\n",
    "            label = tk.Label(frame, text=\"Initializing...\", font=(\"Arial\", 14), fg=\"red\")\n",
    "            label.pack(fill=tk.BOTH, expand=True)\n",
    "            self.camera_labels.append(label)\n",
    "        for i in range(rows):\n",
    "            self.main_frame.grid_rowconfigure(i, weight=1)\n",
    "        for j in range(cols):\n",
    "            self.main_frame.grid_columnconfigure(j, weight=1)\n",
    "\n",
    "    def start_video_threads(self):\n",
    "        \"\"\"\n",
    "        Starts the appropriate thread for each camera:\n",
    "        - Camera 0 runs update_camera (model1 inference on specific video).\n",
    "        - Camera 1 runs update_camera_yolo (YOLO detection on specific video).\n",
    "        - All other cameras are disabled.\n",
    "        \"\"\"\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                threading.Thread(target=self.update_camera, args=(i,), daemon=True).start()\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                threading.Thread(target=self.update_camera_yolo, args=(i,), daemon=True).start()\n",
    "            else:\n",
    "                self.camera_labels[i].config(text=\"Camera Disabled\", font=(\"Arial\", 16), fg=\"yellow\")\n",
    "\n",
    "    def predict_violence(self, frames_list):\n",
    "        \"\"\"\n",
    "        Runs model1 prediction over the last SEQUENCE_LENGTH frames.\n",
    "        Returns the predicted class label.\n",
    "        \"\"\"\n",
    "        input_frames = np.array([frames_list[-SEQUENCE_LENGTH:]])\n",
    "        prediction = self.model.predict(input_frames)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        return CLASSES_LIST[predicted_class]\n",
    "\n",
    "    def update_camera(self, index):\n",
    "        \"\"\"\n",
    "        Reads frames from camera 0 (video for model1),\n",
    "        buffers them for sequence prediction,\n",
    "        and logs the result to the report panel if the predicted label is not \"NonViolence\".\n",
    "        \"\"\"\n",
    "        if self.captures[index] is None:\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                # Restart video if reached end.\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "\n",
    "            # Process frame: convert BGR to RGB and resize for display.\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            display_frame = cv2.resize(rgb_frame, (400, 300))\n",
    "            \n",
    "            # Preprocess frame for model1.\n",
    "            proc_frame = preprocess_frame(rgb_frame)\n",
    "            self.frames_buffer.append(proc_frame)\n",
    "            # if len(self.frames_buffer) > SEQUENCE_LENGTH:\n",
    "            self.frames_buffer = self.frames_buffer[-SEQUENCE_LENGTH:]\n",
    "\n",
    "            # Run model1 prediction if enough frames.\n",
    "            # if len(self.frames_buffer) >= SEQUENCE_LENGTH and self.model is not None:\n",
    "            pred_label = self.predict_violence(self.frames_buffer)\n",
    "            current_time = time.time()\n",
    "            # Only log if prediction is not \"NonViolence\"\n",
    "            if pred_label != \"NonViolence\" and (self.last_prediction != pred_label or (current_time - self.last_report_time > 5)):\n",
    "                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                report = f\"[{timestamp}] Prediction (Model1): {pred_label}\"\n",
    "                self.report_listbox.insert(tk.END, report)\n",
    "                self.report_listbox.insert(tk.END, \"-\" * 50)\n",
    "                self.last_prediction = pred_label\n",
    "                self.last_report_time = current_time\n",
    "            # else:\n",
    "            #     cv2.putText(display_frame, \"Loading...\", (10, 25),\n",
    "            #                 cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(display_frame))\n",
    "            self.camera_labels[index].imgtk = img\n",
    "            self.camera_labels[index].config(image=img)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def update_camera_yolo(self, index):\n",
    "        \"\"\"\n",
    "        Reads frames from camera 1 (video for YOLO),\n",
    "        runs object detection using the YOLO model (Ultralytics),\n",
    "        overlays bounding boxes and labels on the frame,\n",
    "        and logs detections to the report panel.\n",
    "        Only detections with confidence >= 0.80 are considered.\n",
    "        \"\"\"\n",
    "        if self.captures[index] is None or self.yolo_model is None:\n",
    "            return\n",
    "\n",
    "        confidence_threshold = 0.80\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "\n",
    "            # Run YOLO detection (Ultralytics returns a list of results for each image)\n",
    "            results = self.yolo_model(frame)[0]  # Get detections for this frame\n",
    "            annotated_frame = frame.copy()\n",
    "            current_detections = []  # List to store detection strings for this frame\n",
    "\n",
    "            # Loop through detected objects\n",
    "            for box in results.boxes:\n",
    "                conf = float(box.conf.cpu().numpy()[0])\n",
    "                # Skip if confidence is below threshold\n",
    "                if conf < confidence_threshold:\n",
    "                    continue\n",
    "                xyxy = box.xyxy.cpu().numpy()[0].astype(int)\n",
    "                cls = int(box.cls.cpu().numpy()[0])\n",
    "                label = self.yolo_names.get(cls, str(cls))\n",
    "                current_detections.append(f\"{label} {conf:.2f}\")\n",
    "                # Draw bounding box and label\n",
    "                # cv2.rectangle(annotated_frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)\n",
    "                # cv2.putText(annotated_frame, f\"{label} {conf:.2f}\", (xyxy[0], xyxy[1]-10),\n",
    "                #             cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Log detections to the report if there is at least one and if conditions are met\n",
    "            if current_detections:\n",
    "                current_time = time.time()\n",
    "                detection_str = \", \".join(current_detections)\n",
    "                if (self.last_yolo_detections != detection_str or \n",
    "                    (current_time - self.last_yolo_report_time > 5)):\n",
    "                    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    self.report_listbox.insert(tk.END, f\"[{timestamp}] YOLO detection: {detection_str}\")\n",
    "                    self.report_listbox.insert(tk.END, \"-\" * 50)\n",
    "                    self.last_yolo_report_time = current_time\n",
    "                    self.last_yolo_detections = detection_str\n",
    "\n",
    "            # Convert annotated_frame from BGR to RGB, resize for display.\n",
    "            display_frame = cv2.resize(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB), (400, 300))\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(display_frame))\n",
    "            self.camera_labels[index].imgtk = img\n",
    "            self.camera_labels[index].config(image=img)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "\n",
    "\n",
    "    def go_back(self):\n",
    "        \"\"\"Resets the UI and returns to the login page.\"\"\"\n",
    "        for widget in self.root.winfo_children():\n",
    "            widget.destroy()\n",
    "        self.__init__(self.root)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'captures'):\n",
    "            for cap in self.captures:\n",
    "                if cap and cap.isOpened():\n",
    "                    cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = SmartMonitoringApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3f1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox, simpledialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tkinter import PhotoImage\n",
    "import numpy as np\n",
    "\n",
    "# Import for model1 (MobileNetV2+biLSTM)\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Import YOLO model from Ultralytics for model2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# === Constants for model1 prediction (MobileNetV2+biLSTM) ===\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64   # Preprocessing dimensions for model1 input\n",
    "SEQUENCE_LENGTH = 16                # Number of frames per prediction sequence\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]\n",
    "COOLDOWN_MODEL1 = 5.0               # Cooldown period (seconds) between model1 inferences\n",
    "\n",
    "# === Constants for YOLO (model2) ===\n",
    "CONFIDENCE_THRESHOLD = 0.80         # Only consider detections above this confidence\n",
    "COOLDOWN_YOLO = 2.0                 # Cooldown period (seconds) between YOLO inferences\n",
    "\n",
    "# Define target camera indices\n",
    "TARGET_CAMERA_MODEL1 = 0            # For model1 (MobileNetV2+biLSTM)\n",
    "TARGET_CAMERA_YOLO   = 1            # For model2 (YOLO)\n",
    "\n",
    "# --- Helper Function for model1 ---\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resize and normalize a single frame for model1 input.\"\"\"\n",
    "    frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "class SmartMonitoringApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Smart Monitoring & Anomaly Detection\")\n",
    "        self.root.state('zoomed')  # Start in full-screen mode\n",
    "\n",
    "        # Load background image\n",
    "        self.bg_image_path = \"assests/111.jpg\"\n",
    "        self.bg_image = Image.open(self.bg_image_path)\n",
    "        self.bg_image = self.bg_image.resize((self.root.winfo_screenwidth(), self.root.winfo_screenheight()))\n",
    "        self.bg_photo = ImageTk.PhotoImage(self.bg_image)\n",
    "        self.bg_label = tk.Label(self.root, image=self.bg_photo)\n",
    "        self.bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "        # Load users\n",
    "        self.users_file = \"users.txt\"\n",
    "        self.users = self.load_users()\n",
    "\n",
    "        # Styles\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        style.configure(\"TFrame\", background=\"black\")\n",
    "        style.configure(\"TLabel\", background=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TEntry\", fieldbackground=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TButton\", background=\"black\", foreground=\"white\")\n",
    "        self.root.configure(bg=\"black\")\n",
    "\n",
    "        # Login UI\n",
    "        self.login_frame = ttk.Frame(self.root, padding=20)\n",
    "        self.login_frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "        self.user_icon = PhotoImage(file=\"assests/icons8-male-user-50.png\")\n",
    "        self.password_icon = PhotoImage(file=\"assests/icons8-password-48.png\")\n",
    "        ttk.Label(self.login_frame, text=\"Username:\", font=(\"Arial\", 14)).grid(row=0, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        ttk.Label(self.login_frame, image=self.user_icon, background=\"black\").grid(row=0, column=1)\n",
    "        self.username_entry = ttk.Entry(self.login_frame, font=(\"Arial\", 14))\n",
    "        self.username_entry.grid(row=0, column=2, padx=10)\n",
    "        ttk.Label(self.login_frame, text=\"Password:\", font=(\"Arial\", 14)).grid(row=1, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        ttk.Label(self.login_frame, image=self.password_icon, background=\"black\").grid(row=1, column=1)\n",
    "        self.password_entry = ttk.Entry(self.login_frame, show=\"*\", font=(\"Arial\", 14))\n",
    "        self.password_entry.grid(row=1, column=2, padx=10)\n",
    "        ttk.Button(self.login_frame, text=\"Login\", command=self.login).grid(row=2, column=1, columnspan=2, pady=15)\n",
    "\n",
    "        # Main frames & Go Back button\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.report_frame = ttk.Frame(root)\n",
    "        self.admin_frame = ttk.Frame(root)\n",
    "        self.go_back_button = ttk.Button(self.root, text=\"Go Back\", command=self.go_back)\n",
    "        self.go_back_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Initialize camera captures\n",
    "        self.num_cameras = 8\n",
    "        self.captures = []\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                cap = cv2.VideoCapture(\"videos/BigFight.mp4\")\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                cap = cv2.VideoCapture(\"videos/cr.mp4\")\n",
    "            else:\n",
    "                cap = None\n",
    "            self.captures.append(cap)\n",
    "        self.camera_labels = []\n",
    "\n",
    "        # Report panel\n",
    "        self.report_listbox = tk.Listbox(self.report_frame, width=50, height=15, font=(\"Arial\", 14), bg=\"black\", fg=\"white\")\n",
    "        self.report_listbox.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Admin panel\n",
    "        self.operator_listbox = tk.Listbox(self.admin_frame, width=50, height=15, font=(\"Arial\", 14), bg=\"black\", fg=\"white\")\n",
    "        self.operator_listbox.pack(fill=tk.BOTH, expand=True)\n",
    "        ttk.Button(self.admin_frame, text=\"Add Operator\", command=self.add_operator).pack(pady=10)\n",
    "        ttk.Button(self.admin_frame, text=\"Delete Operator\", command=self.delete_operator).pack(pady=10)\n",
    "\n",
    "        # Load models\n",
    "        try:\n",
    "            self.model = load_model(\"models/mobileNetv2_biLSTM.h5\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Model1 Load Error\", f\"Failed to load model1: {e}\")\n",
    "            self.model = None\n",
    "        try:\n",
    "            self.yolo_model = YOLO('models/best.pt')\n",
    "            self.yolo_names = self.yolo_model.names\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"YOLO Model Load Error\", f\"Failed to load YOLO model: {e}\")\n",
    "            self.yolo_model = None\n",
    "            self.yolo_names = {}\n",
    "\n",
    "        # Buffers & cooldown timers\n",
    "        self.frames_buffer = []\n",
    "        self.last_report_time = None\n",
    "        self.last_yolo_report_time = None\n",
    "\n",
    "    def load_users(self):\n",
    "        users = {}\n",
    "        try:\n",
    "            with open(self.users_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    u, p, r = line.strip().split(\",\")\n",
    "                    users[u] = {\"password\": p, \"role\": r}\n",
    "        except FileNotFoundError:\n",
    "            with open(self.users_file, \"w\") as f:\n",
    "                f.write(\"admin,admin123,admin\\n\")\n",
    "            users = {\"admin\": {\"password\": \"admin123\", \"role\": \"admin\"}}\n",
    "        return users\n",
    "\n",
    "    def save_user(self, username, password, role):\n",
    "        with open(self.users_file, \"a\") as f:\n",
    "            f.write(f\"{username},{password},{role}\\n\")\n",
    "\n",
    "    def delete_user(self, username):\n",
    "        lines = []\n",
    "        with open(self.users_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        with open(self.users_file, \"w\") as f:\n",
    "            for line in lines:\n",
    "                if not line.startswith(username + \",\"):\n",
    "                    f.write(line)\n",
    "\n",
    "    def login(self):\n",
    "        u = self.username_entry.get()\n",
    "        p = self.password_entry.get()\n",
    "        if not re.match(\"^[A-Za-z]+$\", u):\n",
    "            messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "            return\n",
    "        if len(p) < 8:\n",
    "            messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        if u in self.users and self.users[u][\"password\"] == p:\n",
    "            self.login_frame.destroy()\n",
    "            if self.users[u][\"role\"] == \"admin\":\n",
    "                self.show_admin_interface()\n",
    "            else:\n",
    "                self.show_operator_interface()\n",
    "        else:\n",
    "            messagebox.showerror(\"Login Failed\", \"Invalid username or password\")\n",
    "\n",
    "    def add_operator(self):\n",
    "        username = simpledialog.askstring(\"Add Operator\", \"Enter username:\")\n",
    "        if not username or not re.match(\"^[A-Za-z]+$\", username):\n",
    "            messagebox.showerror(\"Error\", \"Invalid Username\")\n",
    "            return\n",
    "        if username in self.users:\n",
    "            messagebox.showerror(\"Error\", \"Username already exists!\")\n",
    "            return\n",
    "        password = simpledialog.askstring(\"Add Operator\", \"Enter password:\", show=\"*\")\n",
    "        if not password or len(password) < 8:\n",
    "            messagebox.showerror(\"Error\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        self.save_user(username, password, \"operator\")\n",
    "        self.users[username] = {\"password\": password, \"role\": \"operator\"}\n",
    "        self.update_operator_listbox()\n",
    "        messagebox.showinfo(\"Success\", f\"Operator '{username}' added successfully!\")\n",
    "\n",
    "    def update_operator_listbox(self):\n",
    "        self.operator_listbox.delete(0, tk.END)\n",
    "        for u, inf in self.users.items():\n",
    "            if inf[\"role\"] == \"operator\":\n",
    "                self.operator_listbox.insert(tk.END, u)\n",
    "\n",
    "    def delete_operator(self):\n",
    "        sel = self.operator_listbox.curselection()\n",
    "        if not sel:\n",
    "            messagebox.showerror(\"Error\", \"No operator selected!\")\n",
    "            return\n",
    "        username = self.operator_listbox.get(sel)\n",
    "        self.delete_user(username)\n",
    "        del self.users[username]\n",
    "        self.update_operator_listbox()\n",
    "        messagebox.showinfo(\"Success\", f\"Operator '{username}' deleted successfully!\")\n",
    "\n",
    "    def show_admin_interface(self):\n",
    "        self.admin_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.report_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.update_operator_listbox()\n",
    "\n",
    "    def show_operator_interface(self):\n",
    "        self.main_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.report_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.create_camera_grid()\n",
    "        self.start_video_threads()\n",
    "\n",
    "    def create_camera_grid(self):\n",
    "        rows, cols = 4, 2\n",
    "        for i in range(self.num_cameras):\n",
    "            frame = ttk.LabelFrame(self.main_frame, text=f\"Camera {i+1}\")\n",
    "            frame.grid(row=i//cols, column=i%cols, sticky=\"nsew\", padx=10, pady=10)\n",
    "            lbl = tk.Label(frame, text=\"Initializing...\", font=(\"Arial\",14), fg=\"red\")\n",
    "            lbl.pack(fill=tk.BOTH, expand=True)\n",
    "            self.camera_labels.append(lbl)\n",
    "        for r in range(rows): self.main_frame.grid_rowconfigure(r, weight=1)\n",
    "        for c in range(cols): self.main_frame.grid_columnconfigure(c, weight=1)\n",
    "\n",
    "    def start_video_threads(self):\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                threading.Thread(target=self.update_camera, args=(i,), daemon=True).start()\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                threading.Thread(target=self.update_camera_yolo, args=(i,), daemon=True).start()\n",
    "            else:\n",
    "                self.camera_labels[i].config(text=\"Camera Disabled\", font=(\"Arial\",16), fg=\"yellow\")\n",
    "\n",
    "    def predict_violence(self, frames):\n",
    "        input_frames = np.array([frames[-SEQUENCE_LENGTH:]])\n",
    "        preds = self.model.predict(input_frames)\n",
    "        return CLASSES_LIST[np.argmax(preds)]\n",
    "\n",
    "    def update_camera(self, index):\n",
    "        if not self.captures[index] or not self.model:\n",
    "            return\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            disp = cv2.resize(rgb, (400,300))\n",
    "            proc = preprocess_frame(rgb)\n",
    "            self.frames_buffer.append(proc)\n",
    "            self.frames_buffer = self.frames_buffer[-SEQUENCE_LENGTH:]\n",
    "            now = time.time()\n",
    "            # if len(self.frames_buffer) >= SEQUENCE_LENGTH:\n",
    "            if self.last_report_time is None or (now - self.last_report_time) >= COOLDOWN_MODEL1:\n",
    "                    label = self.predict_violence(self.frames_buffer)\n",
    "                    if label != \"NonViolence\":\n",
    "                        ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        self.report_listbox.insert(tk.END, f\"[{ts}] Prediction (Model1): {label}\")\n",
    "                        self.report_listbox.insert(tk.END, \"-\"*50)\n",
    "                    self.last_report_time = now\n",
    "            imgtk = ImageTk.PhotoImage(image=Image.fromarray(disp))\n",
    "            self.camera_labels[index].imgtk = imgtk\n",
    "            self.camera_labels[index].config(image=imgtk)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def update_camera_yolo(self, index):\n",
    "        if not self.captures[index] or not self.yolo_model:\n",
    "            return\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "            now = time.time()\n",
    "            if self.last_yolo_report_time is None or (now - self.last_yolo_report_time) >= COOLDOWN_YOLO:\n",
    "                res = self.yolo_model(frame)[0]\n",
    "                detections = []\n",
    "                for box in res.boxes:\n",
    "                    conf = float(box.conf.cpu().numpy()[0])\n",
    "                    if conf < CONFIDENCE_THRESHOLD:\n",
    "                        continue\n",
    "                    cls = int(box.cls.cpu().numpy()[0])\n",
    "                    name = self.yolo_names.get(cls, str(cls))\n",
    "                    detections.append(f\"{name} {conf:.2f}\")\n",
    "                if detections:\n",
    "                    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    det_str = \", \".join(detections)\n",
    "                    self.report_listbox.insert(tk.END, f\"[{ts}] YOLO detection: {det_str}\")\n",
    "                    self.report_listbox.insert(tk.END, \"-\"*50)\n",
    "                self.last_yolo_report_time = now\n",
    "            disp = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (400,300))\n",
    "            imgtk = ImageTk.PhotoImage(image=Image.fromarray(disp))\n",
    "            self.camera_labels[index].imgtk = imgtk\n",
    "            self.camera_labels[index].config(image=imgtk)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def go_back(self):\n",
    "        for w in self.root.winfo_children():\n",
    "            w.destroy()\n",
    "        self.__init__(self.root)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'captures'):\n",
    "            for cap in self.captures:\n",
    "                if cap and cap.isOpened():\n",
    "                    cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = SmartMonitoringApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776130dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "0: 384x640 1 Accident, 235.2ms\n",
      "Speed: 2.2ms preprocess, 235.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 203.9ms\n",
      "Speed: 1.5ms preprocess, 203.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 201.2ms\n",
      "Speed: 1.6ms preprocess, 201.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 380.0ms\n",
      "Speed: 3.4ms preprocess, 380.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\n",
      "0: 384x640 1 Accident, 200.0ms\n",
      "Speed: 1.2ms preprocess, 200.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\n",
      "0: 384x640 1 Accident, 197.5ms\n",
      "Speed: 1.8ms preprocess, 197.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 210.0ms\n",
      "Speed: 1.2ms preprocess, 210.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 201.0ms\n",
      "Speed: 1.2ms preprocess, 201.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "0: 384x640 2 Accidents, 202.9ms\n",
      "Speed: 1.4ms preprocess, 202.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 201.9ms\n",
      "Speed: 1.2ms preprocess, 201.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\n",
      "0: 384x640 1 Accident, 201.5ms\n",
      "Speed: 1.2ms preprocess, 201.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 204.7ms\n",
      "Speed: 1.2ms preprocess, 204.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 204.0ms\n",
      "Speed: 1.2ms preprocess, 204.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "0: 384x640 1 Accident, 204.1ms\n",
      "Speed: 1.9ms preprocess, 204.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 202.8ms\n",
      "Speed: 1.2ms preprocess, 202.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "0: 384x640 1 Accident, 199.0ms\n",
      "Speed: 1.3ms preprocess, 199.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 204.9ms\n",
      "Speed: 1.7ms preprocess, 204.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 201.0ms\n",
      "Speed: 1.2ms preprocess, 201.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\n",
      "0: 384x640 2 Accidents, 199.1ms\n",
      "Speed: 1.4ms preprocess, 199.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 198.3ms\n",
      "Speed: 1.2ms preprocess, 198.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\n",
      "0: 384x640 1 Accident, 201.3ms\n",
      "Speed: 1.4ms preprocess, 201.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 203.8ms\n",
      "Speed: 1.1ms preprocess, 203.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 202.2ms\n",
      "Speed: 1.5ms preprocess, 202.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\n",
      "0: 384x640 1 Accident, 203.6ms\n",
      "Speed: 1.4ms preprocess, 203.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accidents, 199.4ms\n",
      "Speed: 1.2ms preprocess, 199.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\n",
      "0: 384x640 2 Accidents, 205.4ms\n",
      "Speed: 1.4ms preprocess, 205.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Exception in thread Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "Thread-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\medoo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    self.run()\n",
      "  File \"C:\\Users\\medoo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 892, in run\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 892, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\medoo\\AppData\\Local\\Temp\\ipykernel_32892\\3720200228.py\", line 301, in update_camera_yolo\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\medoo\\AppData\\Local\\Temp\\ipykernel_32892\\3720200228.py\", line 270, in update_camera\n",
      "IndexError: list index out of range\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox, simpledialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tkinter import PhotoImage\n",
    "import numpy as np\n",
    "\n",
    "# Import for model1 (MobileNetV2+biLSTM)\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Import YOLO model from Ultralytics for model2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# === Constants for model1 prediction (MobileNetV2+biLSTM) ===\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64   # Preprocessing dimensions for model1 input\n",
    "SEQUENCE_LENGTH = 16                # Number of frames per prediction sequence\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]\n",
    "COOLDOWN_MODEL1 = 5.0               # Cooldown period (seconds) between model1 inferences\n",
    "CONFIDENCE_THRESHOLD_MODEL1 = 0.80   # Minimum confidence for model1 to report violence\n",
    "\n",
    "# === Constants for YOLO (model2) ===\n",
    "CONFIDENCE_THRESHOLD = 0.80         # Only consider detections above this confidence\n",
    "COOLDOWN_YOLO = 2.0                 # Cooldown period (seconds) between YOLO inferences\n",
    "\n",
    "# Define target camera indices\n",
    "TARGET_CAMERA_MODEL1 = 0            # For model1 (MobileNetV2+biLSTM)\n",
    "TARGET_CAMERA_YOLO   = 1            # For model2 (YOLO)\n",
    "\n",
    "# --- Helper Function for model1 ---\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resize and normalize a single frame for model1 input.\"\"\"\n",
    "    frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "class SmartMonitoringApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Smart Monitoring & Anomaly Detection\")\n",
    "        self.root.state('zoomed')  # Start in full-screen mode\n",
    "\n",
    "        # Load background image\n",
    "        self.bg_image_path = \"assests/111.jpg\"\n",
    "        self.bg_image = Image.open(self.bg_image_path)\n",
    "        self.bg_image = self.bg_image.resize((self.root.winfo_screenwidth(), self.root.winfo_screenheight()))\n",
    "        self.bg_photo = ImageTk.PhotoImage(self.bg_image)\n",
    "        self.bg_label = tk.Label(self.root, image=self.bg_photo)\n",
    "        self.bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "        # Load users\n",
    "        self.users_file = \"users.txt\"\n",
    "        self.users = self.load_users()\n",
    "\n",
    "        # Styles\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        style.configure(\"TFrame\", background=\"black\")\n",
    "        style.configure(\"TLabel\", background=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TEntry\", fieldbackground=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TButton\", background=\"black\", foreground=\"white\")\n",
    "        self.root.configure(bg=\"black\")\n",
    "\n",
    "        # Login UI\n",
    "        self.login_frame = ttk.Frame(self.root, padding=20)\n",
    "        self.login_frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "        self.user_icon = PhotoImage(file=\"assests/icons8-male-user-50.png\")\n",
    "        self.password_icon = PhotoImage(file=\"assests/icons8-password-48.png\")\n",
    "        ttk.Label(self.login_frame, text=\"Username:\", font=(\"Arial\", 14)).grid(row=0, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        ttk.Label(self.login_frame, image=self.user_icon, background=\"black\").grid(row=0, column=1)\n",
    "        self.username_entry = ttk.Entry(self.login_frame, font=(\"Arial\", 14))\n",
    "        self.username_entry.grid(row=0, column=2, padx=10)\n",
    "        ttk.Label(self.login_frame, text=\"Password:\", font=(\"Arial\", 14)).grid(row=1, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        ttk.Label(self.login_frame, image=self.password_icon, background=\"black\").grid(row=1, column=1)\n",
    "        self.password_entry = ttk.Entry(self.login_frame, show=\"*\", font=(\"Arial\", 14))\n",
    "        self.password_entry.grid(row=1, column=2, padx=10)\n",
    "        ttk.Button(self.login_frame, text=\"Login\", command=self.login).grid(row=2, column=1, columnspan=2, pady=15)\n",
    "\n",
    "        # Main frames & Go Back button\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.report_frame = ttk.Frame(root)\n",
    "        self.admin_frame = ttk.Frame(root)\n",
    "        self.go_back_button = ttk.Button(self.root, text=\"Go Back\", command=self.go_back)\n",
    "        self.go_back_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Initialize camera captures\n",
    "        self.num_cameras = 8\n",
    "        self.captures = []\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                cap = cv2.VideoCapture(\"videos/BigFight.mp4\")\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                cap = cv2.VideoCapture(\"videos/cr.mp4\")\n",
    "            else:\n",
    "                cap = None\n",
    "            self.captures.append(cap)\n",
    "        self.camera_labels = []\n",
    "\n",
    "        # Report panel\n",
    "        self.report_listbox = tk.Listbox(self.report_frame, width=50, height=15, font=(\"Arial\", 14), bg=\"black\", fg=\"white\")\n",
    "        self.report_listbox.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Admin panel\n",
    "        self.operator_listbox = tk.Listbox(self.admin_frame, width=50, height=15, font=(\"Arial\", 14), bg=\"black\", fg=\"white\")\n",
    "        self.operator_listbox.pack(fill=tk.BOTH, expand=True)\n",
    "        ttk.Button(self.admin_frame, text=\"Add Operator\", command=self.add_operator).pack(pady=10)\n",
    "        ttk.Button(self.admin_frame, text=\"Delete Operator\", command=self.delete_operator).pack(pady=10)\n",
    "\n",
    "        # Load models\n",
    "        try:\n",
    "            self.model = load_model(\"models/mobileNetv2_biLSTM.h5\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Model1 Load Error\", f\"Failed to load model1: {e}\")\n",
    "            self.model = None\n",
    "        try:\n",
    "            self.yolo_model = YOLO('models/best.pt')\n",
    "            self.yolo_names = self.yolo_model.names\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"YOLO Model Load Error\", f\"Failed to load YOLO model: {e}\")\n",
    "            self.yolo_model = None\n",
    "            self.yolo_names = {}\n",
    "\n",
    "        # Buffers & cooldown timers\n",
    "        self.frames_buffer = []\n",
    "        self.last_report_time = None\n",
    "        self.last_yolo_report_time = None\n",
    "\n",
    "    def load_users(self):\n",
    "        users = {}\n",
    "        try:\n",
    "            with open(self.users_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    u, p, r = line.strip().split(\",\")\n",
    "                    users[u] = {\"password\": p, \"role\": r}\n",
    "        except FileNotFoundError:\n",
    "            with open(self.users_file, \"w\") as f:\n",
    "                f.write(\"admin,admin123,admin\\n\")\n",
    "            users = {\"admin\": {\"password\": \"admin123\", \"role\": \"admin\"}}\n",
    "        return users\n",
    "\n",
    "    def save_user(self, username, password, role):\n",
    "        with open(self.users_file, \"a\") as f:\n",
    "            f.write(f\"{username},{password},{role}\\n\")\n",
    "\n",
    "    def delete_user(self, username):\n",
    "        lines = []\n",
    "        with open(self.users_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        with open(self.users_file, \"w\") as f:\n",
    "            for line in lines:\n",
    "                if not line.startswith(username + \",\"):\n",
    "                    f.write(line)\n",
    "\n",
    "    def login(self):\n",
    "        u = self.username_entry.get()\n",
    "        p = self.password_entry.get()\n",
    "        if not re.match(\"^[A-Za-z]+$\", u):\n",
    "            messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "            return\n",
    "        if len(p) < 8:\n",
    "            messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        if u in self.users and self.users[u][\"password\"] == p:\n",
    "            self.login_frame.destroy()\n",
    "            if self.users[u][\"role\"] == \"admin\":\n",
    "                self.show_admin_interface()\n",
    "            else:\n",
    "                self.show_operator_interface()\n",
    "        else:\n",
    "            messagebox.showerror(\"Login Failed\", \"Invalid username or password\")\n",
    "\n",
    "    def add_operator(self):\n",
    "        username = simpledialog.askstring(\"Add Operator\", \"Enter username:\")\n",
    "        if not username or not re.match(\"^[A-Za-z]+$\", username):\n",
    "            messagebox.showerror(\"Error\", \"Invalid Username\")\n",
    "            return\n",
    "        if username in self.users:\n",
    "            messagebox.showerror(\"Error\", \"Username already exists!\")\n",
    "            return\n",
    "        password = simpledialog.askstring(\"Add Operator\", \"Enter password:\", show=\"*\")\n",
    "        if not password or len(password) < 8:\n",
    "            messagebox.showerror(\"Error\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        self.save_user(username, password, \"operator\")\n",
    "        self.users[username] = {\"password\": password, \"role\": \"operator\"}\n",
    "        self.update_operator_listbox()\n",
    "        messagebox.showinfo(\"Success\", f\"Operator '{username}' added successfully!\")\n",
    "\n",
    "    def update_operator_listbox(self):\n",
    "        self.operator_listbox.delete(0, tk.END)\n",
    "        for u, inf in self.users.items():\n",
    "            if inf[\"role\"] == \"operator\":\n",
    "                self.operator_listbox.insert(tk.END, u)\n",
    "\n",
    "    def delete_operator(self):\n",
    "        sel = self.operator_listbox.curselection()\n",
    "        if not sel:\n",
    "            messagebox.showerror(\"Error\", \"No operator selected!\")\n",
    "            return\n",
    "        username = self.operator_listbox.get(sel)\n",
    "        self.delete_user(username)\n",
    "        del self.users[username]\n",
    "        self.update_operator_listbox()\n",
    "        messagebox.showinfo(\"Success\", f\"Operator '{username}' deleted successfully!\")\n",
    "\n",
    "    def show_admin_interface(self):\n",
    "        self.admin_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.report_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.update_operator_listbox()\n",
    "\n",
    "    def show_operator_interface(self):\n",
    "        self.main_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.report_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        self.create_camera_grid()\n",
    "        self.start_video_threads()\n",
    "\n",
    "    def create_camera_grid(self):\n",
    "        rows, cols = 4, 2\n",
    "        for i in range(self.num_cameras):\n",
    "            frame = ttk.LabelFrame(self.main_frame, text=f\"Camera {i+1}\")\n",
    "            frame.grid(row=i//cols, column=i%cols, sticky=\"nsew\", padx=10, pady=10)\n",
    "            lbl = tk.Label(frame, text=\"Initializing...\", font=(\"Arial\",14), fg=\"red\")\n",
    "            lbl.pack(fill=tk.BOTH, expand=True)\n",
    "            self.camera_labels.append(lbl)\n",
    "        for r in range(rows): self.main_frame.grid_rowconfigure(r, weight=1)\n",
    "        for c in range(cols): self.main_frame.grid_columnconfigure(c, weight=1)\n",
    "\n",
    "    def start_video_threads(self):\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                threading.Thread(target=self.update_camera, args=(i,), daemon=True).start()\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                threading.Thread(target=self.update_camera_yolo, args=(i,), daemon=True).start()\n",
    "            else:\n",
    "                self.camera_labels[i].config(text=\"Camera Disabled\", font=(\"Arial\",16), fg=\"yellow\")\n",
    "\n",
    "    def predict_violence(self, frames):\n",
    "        \"\"\"Run model1 to get label and confidence for the recent frame sequence.\"\"\"\n",
    "        input_frames = np.array([frames[-SEQUENCE_LENGTH:]])\n",
    "        preds = self.model.predict(input_frames)\n",
    "        confidence = float(np.max(preds))\n",
    "        label = CLASSES_LIST[int(np.argmax(preds))]\n",
    "        return label, confidence\n",
    "\n",
    "    def update_camera(self, index):\n",
    "        if not self.captures[index] or not self.model:\n",
    "            return\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            disp = cv2.resize(rgb, (400,300))\n",
    "            proc = preprocess_frame(rgb)\n",
    "            self.frames_buffer.append(proc)\n",
    "            self.frames_buffer = self.frames_buffer[-SEQUENCE_LENGTH:]\n",
    "            now = time.time()\n",
    "            if ( self.last_report_time is None or (now - self.last_report_time) >= COOLDOWN_MODEL1):\n",
    "                label, conf = self.predict_violence(self.frames_buffer)\n",
    "                if label != \"NonViolence\" and conf >= CONFIDENCE_THRESHOLD_MODEL1:\n",
    "                    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    self.report_listbox.insert(tk.END, f\"[{ts}] Prediction (Model1): {label} ({conf:.2f})\")\n",
    "                    self.report_listbox.insert(tk.END, \"-\"*50)\n",
    "                self.last_report_time = now\n",
    "            imgtk = ImageTk.PhotoImage(image=Image.fromarray(disp))\n",
    "            self.camera_labels[index].imgtk = imgtk\n",
    "            self.camera_labels[index].config(image=imgtk)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def update_camera_yolo(self, index):\n",
    "        if not self.captures[index] or not self.yolo_model:\n",
    "            return\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "            now = time.time()\n",
    "            if self.last_yolo_report_time is None or (now - self.last_yolo_report_time) >= COOLDOWN_YOLO:\n",
    "                res = self.yolo_model(frame)[0]\n",
    "                detections = []\n",
    "                for box in res.boxes:\n",
    "                    conf = float(box.conf.cpu().numpy()[0])\n",
    "                    if conf < CONFIDENCE_THRESHOLD:\n",
    "                        continue\n",
    "                    cls = int(box.cls.cpu().numpy()[0])\n",
    "                    name = self.yolo_names.get(cls, str(cls))\n",
    "                    detections.append(f\"{name} {conf:.2f}\")\n",
    "                if detections:\n",
    "                    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    det_str = \", \".join(detections)\n",
    "                    self.report_listbox.insert(tk.END, f\"[{ts}] YOLO detection: {det_str}\")\n",
    "                    self.report_listbox.insert(tk.END, \"-\"*50)\n",
    "                self.last_yolo_report_time = now\n",
    "            disp = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (400,300))\n",
    "            imgtk = ImageTk.PhotoImage(image=Image.fromarray(disp))\n",
    "            self.camera_labels[index].imgtk = imgtk\n",
    "            self.camera_labels[index].config(image=imgtk)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def go_back(self):\n",
    "        for w in self.root.winfo_children():\n",
    "            w.destroy()\n",
    "        self.__init__(self.root)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'captures'):\n",
    "            for cap in self.captures:\n",
    "                if cap and cap.isOpened():\n",
    "                    cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = SmartMonitoringApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
