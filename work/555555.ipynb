{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cee84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox, simpledialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tkinter import PhotoImage\n",
    "import numpy as np\n",
    "\n",
    "# Import the model loader from TensorFlow/Keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Constants for prediction ===\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64   # Preprocessing dimensions for model input\n",
    "SEQUENCE_LENGTH = 16                 # Number of frames per prediction sequence\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]\n",
    "\n",
    "# Define the target camera index which runs the model inference\n",
    "TARGET_CAMERA = 0\n",
    "\n",
    "# --- Helper Function ---\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resize and normalize a single frame for model input.\"\"\"\n",
    "    frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "class SmartMonitoringApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Smart Monitoring & Anomaly Detection\")\n",
    "        self.root.state('zoomed')  # Start in full-screen mode\n",
    "\n",
    "        # Load background image (update path as necessary)\n",
    "        self.bg_image_path = \"assests/111.jpg\"  \n",
    "        self.bg_image = Image.open(self.bg_image_path)\n",
    "        self.bg_image = self.bg_image.resize((self.root.winfo_screenwidth(), self.root.winfo_screenheight()))\n",
    "        self.bg_photo = ImageTk.PhotoImage(self.bg_image)\n",
    "\n",
    "        # Set the background image\n",
    "        self.bg_label = tk.Label(self.root, image=self.bg_photo)\n",
    "        self.bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "        # Load users from a file (for login)\n",
    "        self.users_file = \"users.txt\"\n",
    "        self.users = self.load_users()\n",
    "\n",
    "        # Setup styling for the application\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        style.configure(\"TFrame\", background=\"black\")\n",
    "        style.configure(\"TLabel\", background=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TEntry\", fieldbackground=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TButton\", background=\"black\", foreground=\"white\")\n",
    "        self.root.configure(bg=\"black\")\n",
    "\n",
    "        # Create login frame\n",
    "        self.login_frame = ttk.Frame(self.root, padding=20, style=\"TFrame\")\n",
    "        self.login_frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "\n",
    "        # Load icons for username and password (update paths as needed)\n",
    "        self.user_icon = PhotoImage(file=\"assests/icons8-male-user-50.png\")\n",
    "        self.password_icon = PhotoImage(file=\"assests/icons8-password-48.png\")\n",
    "\n",
    "        # Username and password fields\n",
    "        self.username_label = ttk.Label(self.login_frame, text=\"Username:\", font=(\"Arial\", 14))\n",
    "        self.username_label.grid(row=0, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.user_icon_label = ttk.Label(self.login_frame, image=self.user_icon, background=\"black\")\n",
    "        self.user_icon_label.grid(row=0, column=1, padx=(0, 10))\n",
    "        self.username_entry = ttk.Entry(self.login_frame, font=(\"Arial\", 14))\n",
    "        self.username_entry.grid(row=0, column=2, padx=(0, 10), pady=15)\n",
    "        self.password_label = ttk.Label(self.login_frame, text=\"Password:\", font=(\"Arial\", 14))\n",
    "        self.password_label.grid(row=1, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.password_icon_label = ttk.Label(self.login_frame, image=self.password_icon, background=\"black\")\n",
    "        self.password_icon_label.grid(row=1, column=1, padx=(0, 10))\n",
    "        self.password_entry = ttk.Entry(self.login_frame, show=\"*\", font=(\"Arial\", 14))\n",
    "        self.password_entry.grid(row=1, column=2, padx=(0, 10), pady=15)\n",
    "        self.login_button = ttk.Button(self.login_frame, text=\"Login\", command=self.login)\n",
    "        self.login_button.grid(row=2, column=1, columnspan=2, padx=20, pady=15)\n",
    "\n",
    "        # Main application frames\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.report_frame = ttk.Frame(root)\n",
    "        self.admin_frame = ttk.Frame(root)\n",
    "\n",
    "        # Initialize camera captures for 8 cameras.\n",
    "        # For camera 0 (TARGET_CAMERA): load the specific video for model inference.\n",
    "        # For all other cameras, set to None (disabled).\n",
    "        self.num_cameras = 8\n",
    "        self.captures = []\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA:\n",
    "                # Load the specific video for camera 0 (update the file path accordingly)\n",
    "                cap = cv2.VideoCapture(\"videos/BigFight.mp4\")\n",
    "            else:\n",
    "                cap = None\n",
    "            self.captures.append(cap)\n",
    "\n",
    "        # Create labels to display camera feeds.\n",
    "        self.camera_labels = []\n",
    "\n",
    "        # Report Panel (for prediction logs)\n",
    "        self.report_listbox = tk.Listbox(self.report_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                         background=\"black\", fg=\"white\")\n",
    "        self.report_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Admin/Operator Panel components (if needed)\n",
    "        self.operator_listbox = tk.Listbox(self.admin_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                           background=\"black\", fg=\"white\")\n",
    "        self.operator_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "        self.add_operator_button = ttk.Button(self.admin_frame, text=\"Add Operator\", command=self.add_operator)\n",
    "        self.add_operator_button.pack(padx=10, pady=10)\n",
    "        self.delete_operator_button = ttk.Button(self.admin_frame, text=\"Delete Operator\", command=self.delete_operator)\n",
    "        self.delete_operator_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # \"Go Back\" button to return to the login interface.\n",
    "        self.go_back_button = ttk.Button(self.root, text=\"Go Back\", command=self.go_back)\n",
    "        self.go_back_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Load the model (MobileNetV2+biLSTM) from file.\n",
    "        try:\n",
    "            self.model = load_model(\"models/mobileNetv2_biLSTM.h5\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Model Load Error\", f\"Failed to load model: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "        # Variables for prediction logging and frame buffering.\n",
    "        self.last_prediction = None\n",
    "        self.last_report_time = 0\n",
    "        self.frames_buffer = []  # Buffer for the last SEQUENCE_LENGTH preprocessed frames\n",
    "\n",
    "    def load_users(self):\n",
    "        \"\"\"Loads users from file into a dictionary.\"\"\"\n",
    "        users = {}\n",
    "        try:\n",
    "            with open(self.users_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    username, password, role = line.strip().split(\",\")\n",
    "                    users[username] = {\"password\": password, \"role\": role}\n",
    "        except FileNotFoundError:\n",
    "            with open(self.users_file, \"w\") as file:\n",
    "                file.write(\"admin,admin123,admin\\n\")\n",
    "            users = {\"admin\": {\"password\": \"admin123\", \"role\": \"admin\"}}\n",
    "        return users\n",
    "\n",
    "    def save_user(self, username, password, role):\n",
    "        \"\"\"Saves a new user to file.\"\"\"\n",
    "        with open(self.users_file, \"a\") as file:\n",
    "            file.write(f\"{username},{password},{role}\\n\")\n",
    "\n",
    "    def delete_user(self, username):\n",
    "        \"\"\"Deletes a user from file.\"\"\"\n",
    "        with open(self.users_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        with open(self.users_file, \"w\") as file:\n",
    "            for line in lines:\n",
    "                if not line.startswith(username + \",\"):\n",
    "                    file.write(line)\n",
    "\n",
    "    def login(self):\n",
    "        username = self.username_entry.get()\n",
    "        password = self.password_entry.get()\n",
    "        if not re.match(\"^[A-Za-z]+$\", username):\n",
    "            messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "            return\n",
    "        if len(password) < 8:\n",
    "            messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        if username in self.users and self.users[username][\"password\"] == password:\n",
    "            self.login_frame.destroy()\n",
    "            if self.users[username][\"role\"] == \"admin\":\n",
    "                self.show_admin_interface()\n",
    "            else:\n",
    "                self.show_operator_interface()\n",
    "        else:\n",
    "            messagebox.showerror(\"Login Failed\", \"Invalid username or password\")\n",
    "\n",
    "    def add_operator(self):\n",
    "        username = simpledialog.askstring(\"Add Operator\", \"Enter username:\")\n",
    "        if username:\n",
    "            if not re.match(\"^[A-Za-z]+$\", username):\n",
    "                messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "                return\n",
    "            if username in self.users:\n",
    "                messagebox.showerror(\"Error\", \"Username already exists!\")\n",
    "                return\n",
    "            password = simpledialog.askstring(\"Add Operator\", \"Enter password:\", show=\"*\")\n",
    "            if password:\n",
    "                if len(password) < 8:\n",
    "                    messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "                    return\n",
    "                self.save_user(username, password, \"operator\")\n",
    "                self.users[username] = {\"password\": password, \"role\": \"operator\"}\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' added successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Password cannot be empty!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Username cannot be empty!\")\n",
    "\n",
    "    def update_operator_listbox(self):\n",
    "        self.operator_listbox.delete(0, tk.END)\n",
    "        for username, info in self.users.items():\n",
    "            if info[\"role\"] == \"operator\":\n",
    "                self.operator_listbox.insert(tk.END, username)\n",
    "\n",
    "    def delete_operator(self):\n",
    "        selected = self.operator_listbox.curselection()\n",
    "        if selected:\n",
    "            username = self.operator_listbox.get(selected)\n",
    "            if username in self.users:\n",
    "                self.delete_user(username)\n",
    "                del self.users[username]\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' deleted successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Operator not found!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"No operator selected!\")\n",
    "\n",
    "    def show_admin_interface(self):\n",
    "        self.admin_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.update_operator_listbox()\n",
    "\n",
    "    def show_operator_interface(self):\n",
    "        self.main_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.create_camera_grid()\n",
    "        self.start_video_threads()\n",
    "\n",
    "    def create_camera_grid(self):\n",
    "        \"\"\"Creates a grid layout for displaying camera feeds.\"\"\"\n",
    "        rows, cols = 4, 2\n",
    "        for i in range(self.num_cameras):\n",
    "            frame = ttk.LabelFrame(self.main_frame, text=f\"Camera {i+1}\")\n",
    "            frame.grid(row=i // cols, column=i % cols, padx=10, pady=10, sticky=\"nsew\")\n",
    "            label = tk.Label(frame, text=\"Initializing...\", font=(\"Arial\", 14), fg=\"red\")\n",
    "            label.pack(fill=tk.BOTH, expand=True)\n",
    "            self.camera_labels.append(label)\n",
    "        for i in range(rows):\n",
    "            self.main_frame.grid_rowconfigure(i, weight=1)\n",
    "        for j in range(cols):\n",
    "            self.main_frame.grid_columnconfigure(j, weight=1)\n",
    "\n",
    "    def start_video_threads(self):\n",
    "        \"\"\"\n",
    "        Starts the appropriate thread for each camera:\n",
    "        - Camera 0 (TARGET_CAMERA) runs update_camera (with model inference using the specific video).\n",
    "        - All other cameras are disabled.\n",
    "        \"\"\"\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA:\n",
    "                threading.Thread(target=self.update_camera, args=(i,), daemon=True).start()\n",
    "            else:\n",
    "                self.camera_labels[i].config(text=\"Camera Disabled\", font=(\"Arial\", 16), fg=\"yellow\")\n",
    "\n",
    "    def predict_violence(self, frames_list):\n",
    "        \"\"\"\n",
    "        Runs model prediction over the last SEQUENCE_LENGTH frames.\n",
    "        Returns the predicted class label.\n",
    "        \"\"\"\n",
    "        input_frames = np.array([frames_list[-SEQUENCE_LENGTH:]])\n",
    "        prediction = self.model.predict(input_frames)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        return CLASSES_LIST[predicted_class]\n",
    "\n",
    "    def update_camera(self, index):\n",
    "        \"\"\"\n",
    "        Reads frames from camera 0 (the specific video),\n",
    "        buffers them for sequence prediction,\n",
    "        overlays prediction text on the display frame,\n",
    "        and logs the result to the report panel.\n",
    "        \n",
    "        *Note:* Predictions with label \"NonViolence\" are not logged.\n",
    "        \"\"\"\n",
    "        if self.captures[index] is None:\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                # Restart video if we reach the end.\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "\n",
    "            # Convert BGR to RGB and resize for display.\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            display_frame = cv2.resize(rgb_frame, (400, 300))\n",
    "            \n",
    "            # Preprocess the frame for prediction.\n",
    "            proc_frame = preprocess_frame(rgb_frame)\n",
    "            self.frames_buffer.append(proc_frame)\n",
    "            if len(self.frames_buffer) > SEQUENCE_LENGTH:\n",
    "                self.frames_buffer = self.frames_buffer[-SEQUENCE_LENGTH:]\n",
    "\n",
    "            # Run prediction if enough frames exist.\n",
    "            if len(self.frames_buffer) >= SEQUENCE_LENGTH and self.model is not None:\n",
    "                pred_label = self.predict_violence(self.frames_buffer)\n",
    "                # cv2.putText(display_frame, f\"Prediction: {pred_label}\", (10, 25),\n",
    "                #             cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "                current_time = time.time()\n",
    "                # Only log predictions if they are not \"NonViolence\"\n",
    "                # and if the prediction changed or 5 seconds have elapsed.\n",
    "                if pred_label != \"NonViolence\" and (self.last_prediction != pred_label or (current_time - self.last_report_time > 5)):\n",
    "                    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    report = f\"[{timestamp}] Prediction: {pred_label}\"\n",
    "                    self.report_listbox.insert(tk.END, report)\n",
    "                    self.report_listbox.insert(tk.END, \"-\" * 50)\n",
    "                    self.last_prediction = pred_label\n",
    "                    self.last_report_time = current_time\n",
    "            else:\n",
    "                cv2.putText(display_frame, \"Loading...\", (10, 25),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(display_frame))\n",
    "            self.camera_labels[index].imgtk = img\n",
    "            self.camera_labels[index].config(image=img)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def go_back(self):\n",
    "        \"\"\"Resets the UI and returns to the login page.\"\"\"\n",
    "        for widget in self.root.winfo_children():\n",
    "            widget.destroy()\n",
    "        self.__init__(self.root)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'captures'):\n",
    "            for cap in self.captures:\n",
    "                if cap and cap.isOpened():\n",
    "                    cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = SmartMonitoringApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\medoo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 892, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\medoo\\AppData\\Local\\Temp\\ipykernel_36240\\3271281467.py\", line 336, in update_camera\n",
      "UnboundLocalError: local variable 'display_frame' referenced before assignment\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\medoo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\medoo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 892, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\medoo\\AppData\\Local\\Temp\\ipykernel_36240\\3271281467.py\", line 336, in update_camera\n",
      "UnboundLocalError: local variable 'display_frame' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Camera 1 (index 0) to 'Camera Disabled'\n",
      "Setting Camera 2 (index 1) to 'Camera Disabled'\n",
      "Setting Camera 3 (index 2) to 'Camera Disabled'\n",
      "Setting Camera 4 (index 3) to 'Camera Disabled'\n",
      "Setting Camera 5 (index 4) to 'Camera Disabled'\n",
      "Setting Camera 6 (index 5) to 'Camera Disabled'\n",
      "Starting thread for Camera 7 (index 6)\n",
      "Starting thread for Camera 8 (index 7)\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import tkinter as tk\n",
    "# from tkinter import ttk, messagebox, simpledialog\n",
    "# from PIL import Image, ImageTk\n",
    "# import threading\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# import re\n",
    "# from tkinter import PhotoImage\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "# import pandas as pd\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Constants for prediction\n",
    "# IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64   # Preprocessing dimensions for sequence model input\n",
    "# SEQUENCE_LENGTH = 16                 # Number of frames per prediction sequence for sequence model\n",
    "# CLASSES_LIST = [\"NonViolence\", \"Violence\"]\n",
    "\n",
    "# # Helper Function\n",
    "# def preprocess_frame(frame):\n",
    "#     \"\"\"Resize and normalize a single frame for sequence model input.\"\"\"\n",
    "#     frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "#     frame = frame / 255.0\n",
    "#     return frame\n",
    "\n",
    "# class SmartMonitoringApp:\n",
    "#     def __init__(self, root):\n",
    "#         self.root = root\n",
    "#         self.root.title(\"Smart Monitoring & Anomaly Detection\")\n",
    "#         self.root.state('zoomed')  # Start in full-screen mode\n",
    "\n",
    "#         # Load background image (update path as necessary)\n",
    "#         self.bg_image_path = \"assests/111.jpg\"  \n",
    "#         self.bg_image = Image.open(self.bg_image_path)\n",
    "#         self.bg_image = self.bg_image.resize((self.root.winfo_screenwidth(), self.root.winfo_screenheight()))\n",
    "#         self.bg_photo = ImageTk.PhotoImage(self.bg_image)\n",
    "\n",
    "#         # Set the background image\n",
    "#         self.bg_label = tk.Label(self.root, image=self.bg_photo)\n",
    "#         self.bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "#         # Load users from a file (for login)\n",
    "#         self.users_file = \"users.txt\"\n",
    "#         self.users = self.load_users()\n",
    "\n",
    "#         # Setup styling for the application\n",
    "#         style = ttk.Style()\n",
    "#         style.theme_use('clam')\n",
    "#         style.configure(\"TFrame\", background=\"black\")\n",
    "#         style.configure(\"TLabel\", background=\"black\", foreground=\"white\")\n",
    "#         style.configure(\"TEntry\", fieldbackground=\"black\", foreground=\"white\")\n",
    "#         style.configure(\"TButton\", background=\"black\", foreground=\"white\")\n",
    "#         self.root.configure(bg=\"black\")\n",
    "\n",
    "#         # Create login frame\n",
    "#         self.login_frame = ttk.Frame(self.root, padding=20, style=\"TFrame\")\n",
    "#         self.login_frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "\n",
    "#         # Load icons for username and password (update paths as needed)\n",
    "#         self.user_icon = PhotoImage(file=\"assests/icons8-male-user-50.png\")\n",
    "#         self.password_icon = PhotoImage(file=\"assests/icons8-password-48.png\")\n",
    "\n",
    "#         # Username and password fields\n",
    "#         self.username_label = ttk.Label(self.login_frame, text=\"Username:\", font=(\"Arial\", 14))\n",
    "#         self.username_label.grid(row=0, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "#         self.user_icon_label = ttk.Label(self.login_frame, image=self.user_icon, background=\"black\")\n",
    "#         self.user_icon_label.grid(row=0, column=1, padx=(0, 10))\n",
    "#         self.username_entry = ttk.Entry(self.login_frame, font=(\"Arial\", 14))\n",
    "#         self.username_entry.grid(row=0, column=2, padx=(0, 10), pady=15)\n",
    "#         self.password_label = ttk.Label(self.login_frame, text=\"Password:\", font=(\"Arial\", 14))\n",
    "#         self.password_label.grid(row=1, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "#         self.password_icon_label = ttk.Label(self.login_frame, image=self.password_icon, background=\"black\")\n",
    "#         self.password_icon_label.grid(row=1, column=1, padx=(0, 10))\n",
    "#         self.password_entry = ttk.Entry(self.login_frame, show=\"*\", font=(\"Arial\", 14))\n",
    "#         self.password_entry.grid(row=1, column=2, padx=(0, 10), pady=15)\n",
    "#         self.login_button = ttk.Button(self.login_frame, text=\"Login\", command=self.login)\n",
    "#         self.login_button.grid(row=2, column=1, columnspan=2, padx=20, pady=15)\n",
    "\n",
    "#         # Main application frames\n",
    "#         self.main_frame = ttk.Frame(root)\n",
    "#         self.report_frame = ttk.Frame(root)\n",
    "#         self.admin_frame = ttk.Frame(root)\n",
    "\n",
    "#         # Initialize camera captures and models for 8 cameras\n",
    "#         self.num_cameras = 8\n",
    "#         self.captures = []\n",
    "#         self.models = [None] * self.num_cameras\n",
    "#         self.model_types = [None] * self.num_cameras\n",
    "#         self.class_list = None\n",
    "\n",
    "#         # Load models and set model types\n",
    "#         for i in range(self.num_cameras):\n",
    "#             if i == 0:\n",
    "#                 try:\n",
    "#                     self.models[i] = load_model(\"models/mobileNetv2_biLSTM.h5\")\n",
    "#                     self.model_types[i] = \"sequence\"\n",
    "#                 except Exception as e:\n",
    "#                     messagebox.showerror(\"Model Load Error\", f\"Failed to load model for camera 0: {e}\")\n",
    "#             elif i == 1:\n",
    "#                 try:\n",
    "#                     self.models[i] = YOLO(\"models/best.pt\")  # Load YOLO model as in the snippet\n",
    "#                     self.model_types[i] = \"yolo\"\n",
    "#                     with open(\"models/coco1.txt\", \"r\") as my_file:\n",
    "#                         data = my_file.read()\n",
    "#                         self.class_list = data.split(\"\\n\")\n",
    "#                 except Exception as e:\n",
    "#                     messagebox.showerror(\"Model Load Error\", f\"Failed to load YOLO model for camera 1: {e}\")\n",
    "#             else:\n",
    "#                 self.captures.append(None)\n",
    "\n",
    "#         # Set up captures\n",
    "#         for i in range(self.num_cameras):\n",
    "#             if i == 0:\n",
    "#                 cap = cv2.VideoCapture(\"videos/BigFight.mp4\")\n",
    "#                 if not cap.isOpened():\n",
    "#                     print(f\"Error: Could not open 'videos/BigFight.mp4' for Camera 1\")\n",
    "#                     cap = None\n",
    "#             elif i == 1:\n",
    "#                 cap = cv2.VideoCapture(\"videos/cr.mp4\")\n",
    "#                 if not cap.isOpened():\n",
    "#                     print(f\"Error: Could not open 'videos/carnorm.mp4' for Camera 2\")\n",
    "#                     cap = None\n",
    "#             else:\n",
    "#                 cap = None\n",
    "#             self.captures.append(cap)\n",
    "\n",
    "#         # Create labels to display camera feeds\n",
    "#         self.camera_labels = []\n",
    "\n",
    "#         # Report Panel (for prediction logs)\n",
    "#         self.report_listbox = tk.Listbox(self.report_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "#                                          background=\"black\", fg=\"white\")\n",
    "#         self.report_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "#         # Admin/Operator Panel components\n",
    "#         self.operator_listbox = tk.Listbox(self.admin_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "#                                            background=\"black\", fg=\"white\")\n",
    "#         self.operator_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "#         self.add_operator_button = ttk.Button(self.admin_frame, text=\"Add Operator\", command=self.add_operator)\n",
    "#         self.add_operator_button.pack(padx=10, pady=10)\n",
    "#         self.delete_operator_button = ttk.Button(self.admin_frame, text=\"Delete Operator\", command=self.delete_operator)\n",
    "#         self.delete_operator_button.pack(padx=10, pady=10)\n",
    "\n",
    "#         # \"Go Back\" button\n",
    "#         self.go_back_button = ttk.Button(self.root, text=\"Go Back\", command=self.go_back)\n",
    "#         self.go_back_button.pack(padx=10, pady=10)\n",
    "\n",
    "#         # Per-camera buffers and logging variables\n",
    "#         self.frames_buffers = [[] for _ in range(self.num_cameras)]\n",
    "#         self.last_predictions = [None] * self.num_cameras\n",
    "#         self.last_report_times = [0] * self.num_cameras\n",
    "\n",
    "#     def load_users(self):\n",
    "#         \"\"\"Loads users from file into a dictionary.\"\"\"\n",
    "#         users = {}\n",
    "#         try:\n",
    "#             with open(self.users_file, \"r\") as file:\n",
    "#                 for line in file:\n",
    "#                     username, password, role = line.strip().split(\",\")\n",
    "#                     users[username] = {\"password\": password, \"role\": role}\n",
    "#         except FileNotFoundError:\n",
    "#             with open(self.users_file, \"w\") as file:\n",
    "#                 file.write(\"admin,admin123,admin\\n\")\n",
    "#             users = {\"admin\": {\"password\": \"admin123\", \"role\": \"admin\"}}\n",
    "#         return users\n",
    "\n",
    "#     def save_user(self, username, password, role):\n",
    "#         \"\"\"Saves a new user to file.\"\"\"\n",
    "#         with open(self.users_file, \"a\") as file:\n",
    "#             file.write(f\"{username},{password},{role}\\n\")\n",
    "\n",
    "#     def delete_user(self, username):\n",
    "#         \"\"\"Deletes a user from file.\"\"\"\n",
    "#         with open(self.users_file, \"r\") as file:\n",
    "#             lines = file.readlines()\n",
    "#         with open(self.users_file, \"w\") as file:\n",
    "#             for line in lines:\n",
    "#                 if not line.startswith(username + \",\"):\n",
    "#                     file.write(line)\n",
    "\n",
    "#     def login(self):\n",
    "#         username = self.username_entry.get()\n",
    "#         password = self.password_entry.get()\n",
    "#         if not re.match(\"^[A-Za-z]+$\", username):\n",
    "#             messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "#             return\n",
    "#         if len(password) < 8:\n",
    "#             messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "#             return\n",
    "#         if username in self.users and self.users[username][\"password\"] == password:\n",
    "#             self.login_frame.destroy()\n",
    "#             if self.users[username][\"role\"] == \"admin\":\n",
    "#                 self.show_admin_interface()\n",
    "#             else:\n",
    "#                 self.show_operator_interface()\n",
    "#         else:\n",
    "#             messagebox.showerror(\"Login Failed\", \"Invalid username or password\")\n",
    "\n",
    "#     def add_operator(self):\n",
    "#         username = simpledialog.askstring(\"Add Operator\", \"Enter username:\")\n",
    "#         if username:\n",
    "#             if not re.match(\"^[A-Za-z]+$\", username):\n",
    "#                 messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "#                 return\n",
    "#             if username in self.users:\n",
    "#                 messagebox.showerror(\"Error\", \"Username already exists!\")\n",
    "#                 return\n",
    "#             password = simpledialog.askstring(\"Add Operator\", \"Enter password:\", show=\"*\")\n",
    "#             if password:\n",
    "#                 if len(password) < 8:\n",
    "#                     messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "#                     return\n",
    "#                 self.save_user(username, password, \"operator\")\n",
    "#                 self.users[username] = {\"password\": password, \"role\": \"operator\"}\n",
    "#                 self.update_operator_listbox()\n",
    "#                 messagebox.showinfo(\"Success\", f\"Operator '{username}' added successfully!\")\n",
    "#             else:\n",
    "#                 messagebox.showerror(\"Error\", \"Password cannot be empty!\")\n",
    "#         else:\n",
    "#             messagebox.showerror(\"Error\", \"Username cannot be empty!\")\n",
    "\n",
    "#     def update_operator_listbox(self):\n",
    "#         self.operator_listbox.delete(0, tk.END)\n",
    "#         for username, info in self.users.items():\n",
    "#             if info[\"role\"] == \"operator\":\n",
    "#                 self.operator_listbox.insert(tk.END, username)\n",
    "\n",
    "#     def delete_operator(self):\n",
    "#         selected = self.operator_listbox.curselection()\n",
    "#         if selected:\n",
    "#             username = self.operator_listbox.get(selected)\n",
    "#             if username in self.users:\n",
    "#                 self.delete_user(username)\n",
    "#                 del self.users[username]\n",
    "#                 self.update_operator_listbox()\n",
    "#                 messagebox.showinfo(\"Success\", f\"Operator '{username}' deleted successfully!\")\n",
    "#             else:\n",
    "#                 messagebox.showerror(\"Error\", \"Operator not found!\")\n",
    "#         else:\n",
    "#             messagebox.showerror(\"Error\", \"No operator selected!\")\n",
    "\n",
    "#     def show_admin_interface(self):\n",
    "#         self.admin_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "#         self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "#         self.update_operator_listbox()\n",
    "\n",
    "#     def show_operator_interface(self):\n",
    "#         self.main_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "#         self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "#         self.create_camera_grid()\n",
    "#         self.start_video_threads()\n",
    "\n",
    "#     def create_camera_grid(self):\n",
    "#         \"\"\"Creates a grid layout for displaying camera feeds.\"\"\"\n",
    "#         rows, cols = 4, 2\n",
    "#         for i in range(self.num_cameras):\n",
    "#             frame = ttk.LabelFrame(self.main_frame, text=f\"Camera {i+1}\")\n",
    "#             frame.grid(row=i // cols, column=i % cols, padx=10, pady=10, sticky=\"nsew\")\n",
    "#             label = tk.Label(frame, text=\"Initializing...\", font=(\"Arial\", 14), fg=\"red\")\n",
    "#             label.pack(fill=tk.BOTH, expand=True)\n",
    "#             self.camera_labels.append(label)\n",
    "#         for i in range(rows):\n",
    "#             self.main_frame.grid_rowconfigure(i, weight=1)\n",
    "#         for j in range(cols):\n",
    "#             self.main_frame.grid_columnconfigure(j, weight=1)\n",
    "\n",
    "#     def start_video_threads(self):\n",
    "#         \"\"\"Starts threads for cameras with active captures.\"\"\"\n",
    "#         for i in range(self.num_cameras):\n",
    "#             if self.captures[i] is not None:\n",
    "#                 print(f\"Starting thread for Camera {i+1} (index {i})\")\n",
    "#                 threading.Thread(target=self.update_camera, args=(i,), daemon=True).start()\n",
    "#             else:\n",
    "#                 print(f\"Setting Camera {i+1} (index {i}) to 'Camera Disabled'\")\n",
    "#                 self.camera_labels[i].config(text=\"Camera Disabled\", font=(\"Arial\", 16), fg=\"yellow\")\n",
    "\n",
    "#     def predict_violence(self, frames_list, model):\n",
    "#         \"\"\"Runs model prediction for sequence-based models.\"\"\"\n",
    "#         input_frames = np.array([frames_list[-SEQUENCE_LENGTH:]])\n",
    "#         prediction = model.predict(input_frames)\n",
    "#         predicted_class = np.argmax(prediction)\n",
    "#         return CLASSES_LIST[predicted_class]\n",
    "\n",
    "#     def update_camera(self, index):\n",
    "#         \"\"\"Updates the camera feed and runs predictions if applicable.\"\"\"\n",
    "#         if self.captures[index] is None or not self.captures[index].isOpened():\n",
    "#             self.camera_labels[index].config(text=\"Camera Disabled\", font=(\"Arial\", 16), fg=\"yellow\")\n",
    "#             return\n",
    "\n",
    "#         count = 0  # Frame counter for YOLO processing\n",
    "#         while True:\n",
    "#             ret, frame = self.captures[index].read()\n",
    "#             if not ret:\n",
    "#                 self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)  # Loop video\n",
    "#                 continue\n",
    "\n",
    "#             rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#             if self.model_types[index] == \"sequence\":\n",
    "#                 display_frame = cv2.resize(rgb_frame, (400, 300))\n",
    "#                 proc_frame = preprocess_frame(rgb_frame)\n",
    "#                 self.frames_buffers[index].append(proc_frame)\n",
    "#                 if len(self.frames_buffers[index]) > SEQUENCE_LENGTH:\n",
    "#                     self.frames_buffers[index] = self.frames_buffers[index][-SEQUENCE_LENGTH:]\n",
    "#                 if len(self.frames_buffers[index]) >= SEQUENCE_LENGTH and self.models[index] is not None:\n",
    "#                     pred_label = self.predict_violence(self.frames_buffers[index], self.models[index])\n",
    "#                 else:\n",
    "#                     pred_label = \"Loading...\"\n",
    "#                 cv2.putText(display_frame, f\"Prediction: {pred_label}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "#             elif self.model_types[index] == \"yolo\":\n",
    "#                 count += 1\n",
    "#                 if count % 3 != 0:  # Process every 3rd frame as in the snippet\n",
    "#                     continue\n",
    "#                 frame_for_processing = cv2.resize(rgb_frame, (1020, 500))  # Resize as in the snippet\n",
    "#                 results = self.models[index].predict(frame_for_processing)\n",
    "#                 a = results[0].boxes.data\n",
    "#                 px = pd.DataFrame(a).astype(\"float\")\n",
    "#                 for index_row, row in px.iterrows():\n",
    "#                     x1 = int(row[0])\n",
    "#                     y1 = int(row[1])\n",
    "#                     x2 = int(row[2])\n",
    "#                     y2 = int(row[3])\n",
    "#                     confidence = float(row[4])\n",
    "#                     class_id = int(row[5])\n",
    "#                     if confidence < 0.60:  # Confidence threshold as in the snippet\n",
    "#                         continue\n",
    "#                     c = self.class_list[class_id]\n",
    "#                     cv2.rectangle(frame_for_processing, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "#                     # Use cv2.putText instead of cvzone to match dependencies\n",
    "#                     cv2.putText(frame_for_processing, f'{c} {confidence:.2f}', (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "#                 display_frame = cv2.resize(frame_for_processing, (400, 300))\n",
    "\n",
    "#             # Convert to ImageTk\n",
    "#             img = ImageTk.PhotoImage(image=Image.fromarray(display_frame))\n",
    "#             self.camera_labels[index].imgtk = img\n",
    "#             self.camera_labels[index].config(image=img)\n",
    "\n",
    "#             time.sleep(0.03)\n",
    "\n",
    "#     def go_back(self):\n",
    "#         \"\"\"Resets the UI and returns to the login page.\"\"\"\n",
    "#         for widget in self.root.winfo_children():\n",
    "#             widget.destroy()\n",
    "#         self.__init__(self.root)\n",
    "\n",
    "#     def __del__(self):\n",
    "#         if hasattr(self, 'captures'):\n",
    "#             for cap in self.captures:\n",
    "#                 if cap and cap.isOpened():\n",
    "#                     cap.release()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = tk.Tk()\n",
    "#     app = SmartMonitoringApp(root)\n",
    "#     root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fcf7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "0: 384x640 1 Accident, 455.3ms\n",
      "Speed: 10.8ms preprocess, 455.3ms inference, 23.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 319.2ms\n",
      "Speed: 5.3ms preprocess, 319.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 198.1ms\n",
      "Speed: 2.1ms preprocess, 198.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 196.7ms\n",
      "Speed: 2.0ms preprocess, 196.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 197.8ms\n",
      "Speed: 1.9ms preprocess, 197.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 192.5ms\n",
      "Speed: 1.6ms preprocess, 192.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Accident, 197.4ms\n",
      "Speed: 1.8ms preprocess, 197.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step0: 384x640 1 Accident, 214.4ms\n",
      "Speed: 1.6ms preprocess, 214.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "0: 384x640 1 Accident, 226.7ms\n",
      "Speed: 2.1ms preprocess, 226.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "0: 384x640 2 Accidents, 224.5ms\n",
      "Speed: 1.6ms preprocess, 224.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "0: 384x640 1 Accident, 226.5ms\n",
      "Speed: 2.0ms preprocess, 226.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 (no detections), 228.1ms\n",
      "Speed: 1.6ms preprocess, 228.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "0: 384x640 1 Accident, 230.3ms\n",
      "Speed: 2.6ms preprocess, 230.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "0: 384x640 1 Accident, 224.0ms\n",
      "Speed: 2.1ms preprocess, 224.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "0: 384x640 (no detections), 216.3ms\n",
      "Speed: 2.7ms preprocess, 216.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 1 Accident, 220.2ms\n",
      "Speed: 1.8ms preprocess, 220.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "0: 384x640 1 Accident, 243.3ms\n",
      "Speed: 3.1ms preprocess, 243.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 1 Accident, 224.7ms\n",
      "Speed: 1.3ms preprocess, 224.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "0: 384x640 1 Accident, 225.9ms\n",
      "Speed: 2.0ms preprocess, 225.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "0: 384x640 1 Accident, 219.0ms\n",
      "Speed: 1.7ms preprocess, 219.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "0: 384x640 1 Accident, 219.6ms\n",
      "Speed: 1.8ms preprocess, 219.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 2 Accidents, 226.0ms\n",
      "Speed: 2.0ms preprocess, 226.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "0: 384x640 2 Accidents, 219.2ms\n",
      "Speed: 1.5ms preprocess, 219.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "0: 384x640 2 Accidents, 222.8ms\n",
      "Speed: 2.6ms preprocess, 222.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "0: 384x640 2 Accidents, 234.4ms\n",
      "Speed: 1.6ms preprocess, 234.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "0: 384x640 2 Accidents, 226.9ms\n",
      "Speed: 1.6ms preprocess, 226.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 254.5ms\n",
      "Speed: 1.7ms preprocess, 254.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "0: 384x640 2 Accidents, 218.5ms\n",
      "Speed: 1.8ms preprocess, 218.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "0: 384x640 2 Accidents, 225.6ms\n",
      "Speed: 1.8ms preprocess, 225.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 226.8ms\n",
      "Speed: 1.6ms preprocess, 226.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "0: 384x640 2 Accidents, 227.0ms\n",
      "Speed: 2.2ms preprocess, 227.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 229.4ms\n",
      "Speed: 1.9ms preprocess, 229.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "0: 384x640 2 Accidents, 211.2ms\n",
      "Speed: 1.8ms preprocess, 211.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "0: 384x640 2 Accidents, 231.0ms\n",
      "Speed: 1.8ms preprocess, 231.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 226.2ms\n",
      "Speed: 1.9ms preprocess, 226.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step0: 384x640 2 Accidents, 218.8ms\n",
      "Speed: 1.9ms preprocess, 218.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 228.0ms\n",
      "Speed: 3.4ms preprocess, 228.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "0: 384x640 2 Accidents, 232.7ms\n",
      "Speed: 2.8ms preprocess, 232.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0: 384x640 2 Accidents, 236.0ms\n",
      "Speed: 1.9ms preprocess, 236.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "0: 384x640 2 Accidents, 225.7ms\n",
      "Speed: 1.8ms preprocess, 225.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "0: 384x640 2 Accidents, 232.5ms\n",
      "Speed: 1.9ms preprocess, 232.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 208.5ms\n",
      "Speed: 1.9ms preprocess, 208.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0: 384x640 2 Accidents, 225.2ms\n",
      "Speed: 1.8ms preprocess, 225.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "0: 384x640 2 Accidents, 224.5ms\n",
      "Speed: 2.0ms preprocess, 224.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step0: 384x640 2 Accidents, 219.2ms\n",
      "Speed: 1.5ms preprocess, 219.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox, simpledialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tkinter import PhotoImage\n",
    "import numpy as np\n",
    "\n",
    "# Import for model1 (MobileNetV2+biLSTM)\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Import YOLO model from Ultralytics for model2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# === Constants for model1 prediction (MobileNetV2+biLSTM) ===\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64   # Preprocessing dimensions for model1 input\n",
    "SEQUENCE_LENGTH = 16                 # Number of frames per prediction sequence\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]\n",
    "\n",
    "# Define target camera indices\n",
    "TARGET_CAMERA_MODEL1 = 0   # For model1 (MobileNetV2+biLSTM)\n",
    "TARGET_CAMERA_YOLO   = 1   # For model2 (YOLO)\n",
    "\n",
    "# --- Helper Function for model1 ---\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resize and normalize a single frame for model1 input.\"\"\"\n",
    "    frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "class SmartMonitoringApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Smart Monitoring & Anomaly Detection\")\n",
    "        self.root.state('zoomed')  # Start in full-screen mode\n",
    "\n",
    "        # Load background image (update path as necessary)\n",
    "        self.bg_image_path = \"assests/111.jpg\"  \n",
    "        self.bg_image = Image.open(self.bg_image_path)\n",
    "        self.bg_image = self.bg_image.resize((self.root.winfo_screenwidth(), self.root.winfo_screenheight()))\n",
    "        self.bg_photo = ImageTk.PhotoImage(self.bg_image)\n",
    "        self.bg_label = tk.Label(self.root, image=self.bg_photo)\n",
    "        self.bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "        # Load users from a file (for login)\n",
    "        self.users_file = \"users.txt\"\n",
    "        self.users = self.load_users()\n",
    "\n",
    "        # Setup styling for the application\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        style.configure(\"TFrame\", background=\"black\")\n",
    "        style.configure(\"TLabel\", background=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TEntry\", fieldbackground=\"black\", foreground=\"white\")\n",
    "        style.configure(\"TButton\", background=\"black\", foreground=\"white\")\n",
    "        self.root.configure(bg=\"black\")\n",
    "\n",
    "        # Create login frame\n",
    "        self.login_frame = ttk.Frame(self.root, padding=20, style=\"TFrame\")\n",
    "        self.login_frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "\n",
    "        # Load icons for username and password (update paths as needed)\n",
    "        self.user_icon = PhotoImage(file=\"assests/icons8-male-user-50.png\")\n",
    "        self.password_icon = PhotoImage(file=\"assests/icons8-password-48.png\")\n",
    "\n",
    "        # Username and password fields\n",
    "        self.username_label = ttk.Label(self.login_frame, text=\"Username:\", font=(\"Arial\", 14))\n",
    "        self.username_label.grid(row=0, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.user_icon_label = ttk.Label(self.login_frame, image=self.user_icon, background=\"black\")\n",
    "        self.user_icon_label.grid(row=0, column=1, padx=(0, 10))\n",
    "        self.username_entry = ttk.Entry(self.login_frame, font=(\"Arial\", 14))\n",
    "        self.username_entry.grid(row=0, column=2, padx=(0, 10), pady=15)\n",
    "        self.password_label = ttk.Label(self.login_frame, text=\"Password:\", font=(\"Arial\", 14))\n",
    "        self.password_label.grid(row=1, column=0, padx=10, pady=15, sticky=\"w\")\n",
    "        self.password_icon_label = ttk.Label(self.login_frame, image=self.password_icon, background=\"black\")\n",
    "        self.password_icon_label.grid(row=1, column=1, padx=(0, 10))\n",
    "        self.password_entry = ttk.Entry(self.login_frame, show=\"*\", font=(\"Arial\", 14))\n",
    "        self.password_entry.grid(row=1, column=2, padx=(0, 10), pady=15)\n",
    "        self.login_button = ttk.Button(self.login_frame, text=\"Login\", command=self.login)\n",
    "        self.login_button.grid(row=2, column=1, columnspan=2, padx=20, pady=15)\n",
    "\n",
    "        # Main application frames\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.report_frame = ttk.Frame(root)\n",
    "        self.admin_frame = ttk.Frame(root)\n",
    "\n",
    "        # Initialize camera captures for 8 cameras.\n",
    "        # For camera 0: load video for model1; for camera 1: load video for model2.\n",
    "        self.num_cameras = 8\n",
    "        self.captures = []\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                # Video for model1 on camera 0 (update file path accordingly)\n",
    "                cap = cv2.VideoCapture(\"videos/BigFight.mp4\")\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                # Video for YOLO on camera 1 (update file path accordingly)\n",
    "                cap = cv2.VideoCapture(\"videos/cr.mp4\")\n",
    "            else:\n",
    "                cap = None\n",
    "            self.captures.append(cap)\n",
    "\n",
    "        # Create labels to display camera feeds.\n",
    "        self.camera_labels = []\n",
    "\n",
    "        # Report Panel (for logging predictions/detections)\n",
    "        self.report_listbox = tk.Listbox(self.report_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                         background=\"black\", fg=\"white\")\n",
    "        self.report_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Admin/Operator Panel components (if needed)\n",
    "        self.operator_listbox = tk.Listbox(self.admin_frame, width=50, height=15, font=(\"Arial\", 14),\n",
    "                                           background=\"black\", fg=\"white\")\n",
    "        self.operator_listbox.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "        self.add_operator_button = ttk.Button(self.admin_frame, text=\"Add Operator\", command=self.add_operator)\n",
    "        self.add_operator_button.pack(padx=10, pady=10)\n",
    "        self.delete_operator_button = ttk.Button(self.admin_frame, text=\"Delete Operator\", command=self.delete_operator)\n",
    "        self.delete_operator_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # \"Go Back\" button to return to the login interface.\n",
    "        self.go_back_button = ttk.Button(self.root, text=\"Go Back\", command=self.go_back)\n",
    "        self.go_back_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Load model1 (MobileNetV2+biLSTM) from file.\n",
    "        try:\n",
    "            self.model = load_model(\"models/mobileNetv2_biLSTM.h5\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Model1 Load Error\", f\"Failed to load model1: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "        # Load model2 (YOLO) using Ultralytics.\n",
    "        try:\n",
    "            self.yolo_model = YOLO('models/best.pt')\n",
    "            # YOLO model from Ultralytics typically provides model.names (a dict mapping class indices to names)\n",
    "            self.yolo_names = self.yolo_model.names  # This may be a dict: {0: 'person', 1: 'bicycle', ...}\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"YOLO Model Load Error\", f\"Failed to load YOLO model: {e}\")\n",
    "            self.yolo_model = None\n",
    "            self.yolo_names = {}\n",
    "\n",
    "        # Variables for model1 prediction logging and frame buffering.\n",
    "        self.last_prediction = None\n",
    "        self.last_report_time = 0\n",
    "        self.frames_buffer = []  # Buffer for the last SEQUENCE_LENGTH preprocessed frames\n",
    "        # Variables for YOLO model logging.\n",
    "        self.last_yolo_report_time = 0\n",
    "        self.last_yolo_detections = None\n",
    "\n",
    "    def load_users(self):\n",
    "        \"\"\"Loads users from file into a dictionary.\"\"\"\n",
    "        users = {}\n",
    "        try:\n",
    "            with open(self.users_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    username, password, role = line.strip().split(\",\")\n",
    "                    users[username] = {\"password\": password, \"role\": role}\n",
    "        except FileNotFoundError:\n",
    "            with open(self.users_file, \"w\") as file:\n",
    "                file.write(\"admin,admin123,admin\\n\")\n",
    "            users = {\"admin\": {\"password\": \"admin123\", \"role\": \"admin\"}}\n",
    "        return users\n",
    "\n",
    "    def save_user(self, username, password, role):\n",
    "        \"\"\"Saves a new user to file.\"\"\"\n",
    "        with open(self.users_file, \"a\") as file:\n",
    "            file.write(f\"{username},{password},{role}\\n\")\n",
    "\n",
    "    def delete_user(self, username):\n",
    "        \"\"\"Deletes a user from file.\"\"\"\n",
    "        with open(self.users_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        with open(self.users_file, \"w\") as file:\n",
    "            for line in lines:\n",
    "                if not line.startswith(username + \",\"):\n",
    "                    file.write(line)\n",
    "\n",
    "    def login(self):\n",
    "        username = self.username_entry.get()\n",
    "        password = self.password_entry.get()\n",
    "        if not re.match(\"^[A-Za-z]+$\", username):\n",
    "            messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "            return\n",
    "        if len(password) < 8:\n",
    "            messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "            return\n",
    "        if username in self.users and self.users[username][\"password\"] == password:\n",
    "            self.login_frame.destroy()\n",
    "            if self.users[username][\"role\"] == \"admin\":\n",
    "                self.show_admin_interface()\n",
    "            else:\n",
    "                self.show_operator_interface()\n",
    "        else:\n",
    "            messagebox.showerror(\"Login Failed\", \"Invalid username or password\")\n",
    "\n",
    "    def add_operator(self):\n",
    "        username = simpledialog.askstring(\"Add Operator\", \"Enter username:\")\n",
    "        if username:\n",
    "            if not re.match(\"^[A-Za-z]+$\", username):\n",
    "                messagebox.showerror(\"Invalid Username\", \"Username must contain only letters.\")\n",
    "                return\n",
    "            if username in self.users:\n",
    "                messagebox.showerror(\"Error\", \"Username already exists!\")\n",
    "                return\n",
    "            password = simpledialog.askstring(\"Add Operator\", \"Enter password:\", show=\"*\")\n",
    "            if password:\n",
    "                if len(password) < 8:\n",
    "                    messagebox.showerror(\"Invalid Password\", \"Password must be at least 8 characters long.\")\n",
    "                    return\n",
    "                self.save_user(username, password, \"operator\")\n",
    "                self.users[username] = {\"password\": password, \"role\": \"operator\"}\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' added successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Password cannot be empty!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Username cannot be empty!\")\n",
    "\n",
    "    def update_operator_listbox(self):\n",
    "        self.operator_listbox.delete(0, tk.END)\n",
    "        for username, info in self.users.items():\n",
    "            if info[\"role\"] == \"operator\":\n",
    "                self.operator_listbox.insert(tk.END, username)\n",
    "\n",
    "    def delete_user_from_list(self):\n",
    "        selected = self.operator_listbox.curselection()\n",
    "        if selected:\n",
    "            username = self.operator_listbox.get(selected)\n",
    "            if username in self.users:\n",
    "                self.delete_user(username)\n",
    "                del self.users[username]\n",
    "                self.update_operator_listbox()\n",
    "                messagebox.showinfo(\"Success\", f\"Operator '{username}' deleted successfully!\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"Operator not found!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"No operator selected!\")\n",
    "\n",
    "    def delete_operator(self):\n",
    "        self.delete_user_from_list()\n",
    "\n",
    "    def show_admin_interface(self):\n",
    "        self.admin_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.update_operator_listbox()\n",
    "\n",
    "    def show_operator_interface(self):\n",
    "        self.main_frame.pack(padx=20, pady=20, side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        self.report_frame.pack(padx=20, pady=20, side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        self.create_camera_grid()\n",
    "        self.start_video_threads()\n",
    "\n",
    "    def create_camera_grid(self):\n",
    "        \"\"\"Creates a grid layout for displaying camera feeds.\"\"\"\n",
    "        rows, cols = 4, 2\n",
    "        for i in range(self.num_cameras):\n",
    "            frame = ttk.LabelFrame(self.main_frame, text=f\"Camera {i+1}\")\n",
    "            frame.grid(row=i // cols, column=i % cols, padx=10, pady=10, sticky=\"nsew\")\n",
    "            label = tk.Label(frame, text=\"Initializing...\", font=(\"Arial\", 14), fg=\"red\")\n",
    "            label.pack(fill=tk.BOTH, expand=True)\n",
    "            self.camera_labels.append(label)\n",
    "        for i in range(rows):\n",
    "            self.main_frame.grid_rowconfigure(i, weight=1)\n",
    "        for j in range(cols):\n",
    "            self.main_frame.grid_columnconfigure(j, weight=1)\n",
    "\n",
    "    def start_video_threads(self):\n",
    "        \"\"\"\n",
    "        Starts the appropriate thread for each camera:\n",
    "        - Camera 0 runs update_camera (model1 inference on specific video).\n",
    "        - Camera 1 runs update_camera_yolo (YOLO detection on specific video).\n",
    "        - All other cameras are disabled.\n",
    "        \"\"\"\n",
    "        for i in range(self.num_cameras):\n",
    "            if i == TARGET_CAMERA_MODEL1:\n",
    "                threading.Thread(target=self.update_camera, args=(i,), daemon=True).start()\n",
    "            elif i == TARGET_CAMERA_YOLO:\n",
    "                threading.Thread(target=self.update_camera_yolo, args=(i,), daemon=True).start()\n",
    "            else:\n",
    "                self.camera_labels[i].config(text=\"Camera Disabled\", font=(\"Arial\", 16), fg=\"yellow\")\n",
    "\n",
    "    def predict_violence(self, frames_list):\n",
    "        \"\"\"\n",
    "        Runs model1 prediction over the last SEQUENCE_LENGTH frames.\n",
    "        Returns the predicted class label.\n",
    "        \"\"\"\n",
    "        input_frames = np.array([frames_list[-SEQUENCE_LENGTH:]])\n",
    "        prediction = self.model.predict(input_frames)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        return CLASSES_LIST[predicted_class]\n",
    "\n",
    "    def update_camera(self, index):\n",
    "        \"\"\"\n",
    "        Reads frames from camera 0 (video for model1),\n",
    "        buffers them for sequence prediction,\n",
    "        and logs the result to the report panel if the predicted label is not \"NonViolence\".\n",
    "        \"\"\"\n",
    "        if self.captures[index] is None:\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                # Restart video if reached end.\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "\n",
    "            # Process frame: convert BGR to RGB and resize for display.\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            display_frame = cv2.resize(rgb_frame, (400, 300))\n",
    "            \n",
    "            # Preprocess frame for model1.\n",
    "            proc_frame = preprocess_frame(rgb_frame)\n",
    "            self.frames_buffer.append(proc_frame)\n",
    "            # if len(self.frames_buffer) > SEQUENCE_LENGTH:\n",
    "            self.frames_buffer = self.frames_buffer[-SEQUENCE_LENGTH:]\n",
    "\n",
    "            # Run model1 prediction if enough frames.\n",
    "            # if len(self.frames_buffer) >= SEQUENCE_LENGTH and self.model is not None:\n",
    "            pred_label = self.predict_violence(self.frames_buffer)\n",
    "            current_time = time.time()\n",
    "            # Only log if prediction is not \"NonViolence\"\n",
    "            if pred_label != \"NonViolence\" and (self.last_prediction != pred_label or (current_time - self.last_report_time > 5)):\n",
    "                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                report = f\"[{timestamp}] Prediction (Model1): {pred_label}\"\n",
    "                self.report_listbox.insert(tk.END, report)\n",
    "                self.report_listbox.insert(tk.END, \"-\" * 50)\n",
    "                self.last_prediction = pred_label\n",
    "                self.last_report_time = current_time\n",
    "            # else:\n",
    "            #     cv2.putText(display_frame, \"Loading...\", (10, 25),\n",
    "            #                 cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(display_frame))\n",
    "            self.camera_labels[index].imgtk = img\n",
    "            self.camera_labels[index].config(image=img)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    def update_camera_yolo(self, index):\n",
    "        \"\"\"\n",
    "        Reads frames from camera 1 (video for YOLO),\n",
    "        runs object detection using the YOLO model (Ultralytics),\n",
    "        overlays bounding boxes and labels on the frame,\n",
    "        and logs detections to the report panel.\n",
    "        Only detections with confidence >= 0.80 are considered.\n",
    "        \"\"\"\n",
    "        if self.captures[index] is None or self.yolo_model is None:\n",
    "            return\n",
    "\n",
    "        confidence_threshold = 0.80\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.captures[index].read()\n",
    "            if not ret:\n",
    "                self.captures[index].set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "\n",
    "            # Run YOLO detection (Ultralytics returns a list of results for each image)\n",
    "            results = self.yolo_model(frame)[0]  # Get detections for this frame\n",
    "            annotated_frame = frame.copy()\n",
    "            current_detections = []  # List to store detection strings for this frame\n",
    "\n",
    "            # Loop through detected objects\n",
    "            for box in results.boxes:\n",
    "                conf = float(box.conf.cpu().numpy()[0])\n",
    "                # Skip if confidence is below threshold\n",
    "                if conf < confidence_threshold:\n",
    "                    continue\n",
    "                xyxy = box.xyxy.cpu().numpy()[0].astype(int)\n",
    "                cls = int(box.cls.cpu().numpy()[0])\n",
    "                label = self.yolo_names.get(cls, str(cls))\n",
    "                current_detections.append(f\"{label} {conf:.2f}\")\n",
    "                # Draw bounding box and label\n",
    "                # cv2.rectangle(annotated_frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)\n",
    "                # cv2.putText(annotated_frame, f\"{label} {conf:.2f}\", (xyxy[0], xyxy[1]-10),\n",
    "                #             cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Log detections to the report if there is at least one and if conditions are met\n",
    "            if current_detections:\n",
    "                current_time = time.time()\n",
    "                detection_str = \", \".join(current_detections)\n",
    "                if (self.last_yolo_detections != detection_str or \n",
    "                    (current_time - self.last_yolo_report_time > 5)):\n",
    "                    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    self.report_listbox.insert(tk.END, f\"[{timestamp}] YOLO detection: {detection_str}\")\n",
    "                    self.report_listbox.insert(tk.END, \"-\" * 50)\n",
    "                    self.last_yolo_report_time = current_time\n",
    "                    self.last_yolo_detections = detection_str\n",
    "\n",
    "            # Convert annotated_frame from BGR to RGB, resize for display.\n",
    "            display_frame = cv2.resize(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB), (400, 300))\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(display_frame))\n",
    "            self.camera_labels[index].imgtk = img\n",
    "            self.camera_labels[index].config(image=img)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "\n",
    "\n",
    "    def go_back(self):\n",
    "        \"\"\"Resets the UI and returns to the login page.\"\"\"\n",
    "        for widget in self.root.winfo_children():\n",
    "            widget.destroy()\n",
    "        self.__init__(self.root)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'captures'):\n",
    "            for cap in self.captures:\n",
    "                if cap and cap.isOpened():\n",
    "                    cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = SmartMonitoringApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
